{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose \n",
    "### Feature Selection by optimizing FSJaya Algorithm. \n",
    "\n",
    "The challenge is to tweak the present algorithm for feature selection purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "The Datasets selected for evaluation purposes are: \n",
    "- mandelon\n",
    "- musk \n",
    "\n",
    "Both datasets are accesbile from [add the name of the website]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Madelon Dataset\n",
    "\n",
    "- Data type: non-sparse\n",
    "- Number of features: 500\n",
    "- Number of examples and check-sums:\n",
    "      \t     Pos_ex\tNeg_ex\tTot_ex\tCheck_sum\n",
    "             \n",
    "\n",
    "     Train\t 1000\t 1000\t 2000\t488083511.00\n",
    "     \n",
    "\n",
    "     Valid\t  300\t  300\t  600\t146395833.00\n",
    "     \n",
    "\n",
    "     Test\t  900\t  900\t 1800\t439209553.00\n",
    "     \n",
    "\n",
    "     All  \t 2200\t 2200\t 4400\t1073688897.00\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 501 entries, 0 to 500\n",
      "dtypes: float64(1), int64(500)\n",
      "memory usage: 7.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Columns: 501 entries, 0 to 500\n",
      "dtypes: float64(1), int64(500)\n",
      "memory usage: 6.9 MB\n"
     ]
    }
   ],
   "source": [
    "madelon = pd.read_csv('madelon/madelon_train.data', sep=' ', header = None)\n",
    "madelon.info()\n",
    "madelon.head()\n",
    "\n",
    "madelon_test = pd.read_csv('madelon/madelon_test.data', sep=' ', header = None)\n",
    "madelon_test.info()\n",
    "#madelon_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musk Dataset\n",
    "\n",
    "4. Relevant Information:\n",
    "   This dataset describes a set of **92** molecules of which **47** are judged\n",
    "   by human experts to be musks and the remaining 45 molecules are\n",
    "   judged to be non-musks.  The goal is to learn to predict whether\n",
    "   new molecules will be musks or non-musks.  However, the 166 features\n",
    "   that describe these molecules depend upon the exact shape, or\n",
    "   conformation, of the molecule.  Because bonds can rotate, a single\n",
    "   molecule can adopt many different shapes.  To generate this data\n",
    "   set, the low-energy conformations of the molecules were generated\n",
    "   and then filtered to remove highly similar conformations. This left\n",
    "   476 conformations.  Then, a feature vector was extracted that\n",
    "   describes each conformation.\n",
    "\n",
    "   This many-to-one relationship between feature vectors and molecules\n",
    "   is called the **\"multiple instance problem\"**.  When learning a\n",
    "   classifier for this data, the classifier should classify a molecule\n",
    "   as \"musk\" if ANY of its conformations is classified as a musk.  A\n",
    "   molecule should be classified as \"non-musk\" if NONE of its\n",
    "   conformations is classified as a musk.\n",
    "\n",
    "5. Number of Instances  **476**\n",
    "\n",
    "6. Number of Attributes **168** plus the class.\n",
    "\n",
    "7. For Each Attribute:\n",
    "   \n",
    "   Attribute:           Description:\n",
    "   molecule_name:       Symbolic name of each molecule.  Musks have names such\n",
    "                        as MUSK-188.  Non-musks have names such as\n",
    "                        NON-MUSK-jp13.\n",
    "   conformation_name:   Symbolic name of each conformation.  These\n",
    "                        have the format MOL_ISO+CONF, where MOL is the\n",
    "                        molecule number, ISO is the stereoisomer\n",
    "                        number (usually 1), and CONF is the\n",
    "                        conformation number. \n",
    "   f1 through f162:     These are \"distance features\" along rays (see\n",
    "                        paper cited above).  The distances are\n",
    "                        measured in hundredths of Angstroms.  The\n",
    "                        distances may be negative or positive, since\n",
    "                        they are actually measured relative to an\n",
    "                        origin placed along each ray.  The origin was\n",
    "                        defined by a \"consensus musk\" surface that is\n",
    "                        no longer used.  Hence, any experiments with\n",
    "                        the data should treat these feature values as\n",
    "                        lying on an arbitrary continuous scale.  In\n",
    "                        particular, the algorithm should not make any\n",
    "                        use of the zero point or the sign of each\n",
    "                        feature value. \n",
    "   f163:                This is the distance of the oxygen atom in the\n",
    "                        molecule to a designated point in 3-space.\n",
    "                        This is also called OXY-DIS.\n",
    "   f164:                OXY-X: X-displacement from the designated\n",
    "                        point.\n",
    "   f165:                OXY-Y: Y-displacement from the designated\n",
    "                        point.\n",
    "   f166:                OXY-Z: Z-displacement from the designated\n",
    "                        point. \n",
    "   class:               0 => non-musk, 1 => musk\n",
    "\n",
    "   Please note that the molecule_name and conformation_name attributes\n",
    "   should not be used to predict the class.\n",
    "\n",
    "8. Missing Attribute Values: none.\n",
    "\n",
    "9. Class Distribution: \n",
    "   Musks:     47\n",
    "   Non-musks: 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6598 entries, 0 to 6597\n",
      "Columns: 169 entries, 0 to 168\n",
      "dtypes: float64(1), int64(166), object(2)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "musk =  pd.read_csv('musk/clean2.data/clean2.data', header = None)\n",
    "musk.info()\n",
    "musk.head()\n",
    "\n",
    "musk_test =  pd.read_csv('musk/clean1.data/clean1.data', header = None)\n",
    "#musk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data for Classifiers. \n",
    "\n",
    "According to the shared paper [A jaya algorithm based wrapper method for optimal feature selection in supervised classification] data has to be trained using following classifiers: \n",
    "\n",
    "    - NB (Naive Bayes)\n",
    "    - KNN (K Nearest Neighbor)\n",
    "    - LDA \n",
    "    - RT (Regression Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a : Separating features and labels as X, Y variables  (Training Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Musk_X = musk.drop(168, axis = 1)\n",
    "Musk_X = Musk_X.drop(0, axis=1)\n",
    "Musk_X = Musk_X.drop(1, axis=1)\n",
    "Musk_X.columns = np.arange(len(Musk_X.columns))\n",
    "Musk_Y = musk[168]\n",
    "\n",
    "\n",
    "Madelon_X = madelon.drop(500, axis = 1)\n",
    "Y_labels = np.hstack([np.ones(1000), np.zeros(1000)])\n",
    "Madelon_Y = pd.Series(Y_labels.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b : Separating features and labels as X, Y variables  (Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madelon TRAINING \n",
      " dataset: # of features:  500 \n",
      " Number of Measurements:  2000 Y_shape:  (2000,)\n",
      "Madelon TEST \n",
      " dataset: # of features:  500 \n",
      " Number of Measurements:  1800 Y_shape:  (1800,)\n",
      "Musk TRAINING \n",
      " dataset: # of features:  166 \n",
      " Number of Measurements:  6598 Y_shape:  (6598,)\n",
      "Musk TEST \n",
      " dataset: # of features:  166 \n",
      " Number of Measurements:  476 Y_shape:  (476,)\n"
     ]
    }
   ],
   "source": [
    "Musk_x = musk_test.drop(168, axis = 1)\n",
    "Musk_x = Musk_x.drop(0, axis=1)\n",
    "Musk_x = Musk_x.drop(1, axis=1)\n",
    "Musk_x.columns = np.arange(len(Musk_x.columns))\n",
    "Musk_y = musk_test[168]\n",
    "\n",
    "\n",
    "Madelon_x = madelon_test.drop(500, axis = 1)\n",
    "y_labels = np.hstack([np.ones(900), np.zeros(900)])\n",
    "Madelon_y = pd.Series(y_labels.T)\n",
    "len(Musk_x)\n",
    "len(Musk_y)\n",
    "\n",
    "print(\"Madelon TRAINING \\n dataset: # of features: \" , Madelon_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_X.shape[0], \n",
    "      'Y_shape: ', Madelon_Y.shape)\n",
    "\n",
    "print(\"Madelon TEST \\n dataset: # of features: \" , Madelon_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_x.shape[0], \n",
    "      'Y_shape: ', Madelon_y.shape)\n",
    "\n",
    "print(\"Musk TRAINING \\n dataset: # of features: \" , Musk_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_X.shape[0],\n",
    "      'Y_shape: ', Musk_Y.shape)\n",
    "\n",
    "print(\"Musk TEST \\n dataset: # of features: \" , Musk_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_x.shape[0], \n",
    "      'Y_shape: ', Musk_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Classification\n",
    "\n",
    "#### Step 3a: Train Classifiers without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def data_classification(train_x, train_y, test_x = None ,test_y = None):\n",
    "    names = [\"NB\", \"RT\", \"KNN\", \"LDA\"]\n",
    "\n",
    "    classifiers = [\n",
    "        GaussianNB(),\n",
    "        DecisionTreeRegressor(),\n",
    "        KNeighborsClassifier(n_neighbors=10),\n",
    "        LinearDiscriminantAnalysis()\n",
    "    ]\n",
    "    model_return = {}\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        clf.fit(train_x, np.ravel(train_y))\n",
    "        print(clf)\n",
    "        \n",
    "        # TRAIN\n",
    "        #probs = clf.predict_proba(train_x)\n",
    "        pred_ytrain = clf.predict(train_x)\n",
    "        score_ytrain = clf.score(train_x,np.ravel(train_y))\n",
    "        \n",
    "        #TEST\n",
    "        #probs_t = clf.predict_proba(test_x)\n",
    "        pred_ytest = clf.predict(test_x)\n",
    "        score_ytest = clf.score(test_x,np.ravel(test_y))\n",
    "        \n",
    "        model = {'Name' : name,\n",
    "                'Classes': ['cancer', 'no cancer'],\n",
    "                'Date': '2020-10-01',\n",
    "                'Datasets': ['madelon', 'musk'],\n",
    "                'Training_Score' : score_ytrain,\n",
    "                 'Test_Score' : score_ytest\n",
    "                }\n",
    "        \n",
    "        #print(\"classifier %s, train score %.5f \\n\" %(name,score_ytrain))\n",
    "        #print(\"classifier %s, test score %.5f \\n\" %(name,score_ytest))\n",
    "        print(\"classifier %s \" %(name), classification_report(np.ravel(test_y), pred_ytest, digits = 4))\n",
    "        #print(\"probabilty of test\", probs)\n",
    "        model_return[name] = pred_ytrain\n",
    "    return model_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4959    0.4667    0.4808       900\n",
      "         1.0     0.4963    0.5256    0.5105       900\n",
      "\n",
      "    accuracy                         0.4961      1800\n",
      "   macro avg     0.4961    0.4961    0.4957      1800\n",
      "weighted avg     0.4961    0.4961    0.4957      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5039    0.5033    0.5036       900\n",
      "         1.0     0.5039    0.5044    0.5042       900\n",
      "\n",
      "    accuracy                         0.5039      1800\n",
      "   macro avg     0.5039    0.5039    0.5039      1800\n",
      "weighted avg     0.5039    0.5039    0.5039      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5031    0.6244    0.5573       900\n",
      "         1.0     0.5051    0.3833    0.4359       900\n",
      "\n",
      "    accuracy                         0.5039      1800\n",
      "   macro avg     0.5041    0.5039    0.4966      1800\n",
      "weighted avg     0.5041    0.5039    0.4966      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5137    0.5000    0.5068       900\n",
      "         1.0     0.5130    0.5267    0.5197       900\n",
      "\n",
      "    accuracy                         0.5133      1800\n",
      "   macro avg     0.5133    0.5133    0.5132      1800\n",
      "weighted avg     0.5133    0.5133    0.5132      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "madelon_return = data_classification(train_x = Madelon_X, train_y = Madelon_Y, test_x = Madelon_x ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6724    0.8773    0.7613       269\n",
      "         1.0     0.7360    0.4444    0.5542       207\n",
      "\n",
      "    accuracy                         0.6891       476\n",
      "   macro avg     0.7042    0.6609    0.6578       476\n",
      "weighted avg     0.7000    0.6891    0.6712       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7438    0.4424    0.5548       269\n",
      "         1.0     0.5253    0.8019    0.6348       207\n",
      "\n",
      "    accuracy                         0.5987       476\n",
      "   macro avg     0.6345    0.6222    0.5948       476\n",
      "weighted avg     0.6488    0.5987    0.5896       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9004    0.9405    0.9200       269\n",
      "         1.0     0.9179    0.8647    0.8905       207\n",
      "\n",
      "    accuracy                         0.9076       476\n",
      "   macro avg     0.9092    0.9026    0.9053       476\n",
      "weighted avg     0.9080    0.9076    0.9072       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8111    0.8141    0.8126       269\n",
      "         1.0     0.7573    0.7536    0.7554       207\n",
      "\n",
      "    accuracy                         0.7878       476\n",
      "   macro avg     0.7842    0.7839    0.7840       476\n",
      "weighted avg     0.7877    0.7878    0.7878       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "musk_return = data_classification(train_x = Musk_X, train_y = Musk_Y, test_x = Musk_x ,test_y = Musk_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Algorithm for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class feature_optimization:\n",
    "    def __init__(self, trainX, trainY, testX, testY, population, obj_function):\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.testX  = testX\n",
    "        self.testY  = testY\n",
    "        self.P = population\n",
    "        if obj_function == 'ER':\n",
    "            self.objective_function = fx_ER\n",
    "            self.jaya_function = self.jaya_ER\n",
    "        elif obj_function == 'AUC':\n",
    "            self.objective_function = fx_AUC\n",
    "            self.jaya_function = self.jaya_AUC\n",
    "        else:\n",
    "            print('please select the correct fx function-- ER [Error rate] or AUC [Area under the curve]')\n",
    "        \n",
    "        self.dsp_binary_data = generate_binary_dataset(self.P, self.objective_function)\n",
    "        \n",
    "        \n",
    "    def __call__(self):\n",
    "        self.X = self.dsp_binary_data(self.trainX, self.testX, self.trainY, self.testY)\n",
    "        optimized_population = self.jaya_function(self.X)\n",
    "        #print('OP_Features ', optimized_population.shape)\n",
    "        trainOP, testOP = self.optimized_training_test_sets(optimized_population)\n",
    "        return trainOP, testOP\n",
    "        \n",
    "    def __objective_function(self, X_new): \n",
    "        FV = []\n",
    "        for n in range(len(X_new)):\n",
    "            #print('xnew ', X_new.loc[n])\n",
    "            idx ,  = np.where(X_new.loc[n]==1)\n",
    "            \n",
    "            temp_train = self.trainX[idx]\n",
    "            temp_test = self.testX[idx]\n",
    "            \n",
    "           \n",
    "            FV.append(self.objective_function(X_train = temp_train,\n",
    "                          x_test  = temp_test,\n",
    "                          Y_train = self.trainY,\n",
    "                          y_test  = self.testY)\n",
    "                     )\n",
    "\n",
    "        return FV\n",
    "    \n",
    "    def optimized_training_test_sets(self, X):\n",
    "        idx,  = np.where(X.loc[0]==1)\n",
    "        #print(\"feature IDX \", idx)\n",
    "        temp_train = self.trainX[idx]\n",
    "        temp_test = self.testX[idx]\n",
    "        \n",
    "        return temp_train, temp_test\n",
    "   \n",
    "    \n",
    "\n",
    "    def binary_x_new(self,X_new):\n",
    "        '''\n",
    "        PURPOSE: to convert x_new value into binary [0, 1] \n",
    "        INPUT : output of Jaya algorithm line 36\n",
    "        OUTPUT: binary value of x_new\n",
    "        '''\n",
    "        #probability_jaya_x  = 1/(1 + np.exp(-2 * X_new) - 0.25)   # OUR PROPOSED ONE \n",
    "        probability_jaya_x  = 1/(1 + np.exp(-10 * X_new) - 0.5)    # PAPER FUNCTION\n",
    "        #print('probability_jaya_x ', probability_jaya_x)\n",
    "        random_r =  np.round(np.random.random(1),2)\n",
    "        prob = []\n",
    "        for p in probability_jaya_x:\n",
    "            if p > random_r:\n",
    "                prob.append(1.0)\n",
    "            else:\n",
    "                prob.append(0.0)\n",
    "        return prob    \n",
    "\n",
    "    def jaya_ER(self,x):\n",
    "        '''\n",
    "        INPUT:\n",
    "        bextX is with the smaller fx value, \n",
    "        worstX, \n",
    "        r1 and r2 are two random numbers between 0 and 1 \n",
    "        '''\n",
    "\n",
    "        MaxIter = 100\n",
    "        print('Processing .....')\n",
    "        for s in range(MaxIter):      # line 6\n",
    "            if len(x) >1:\n",
    "                \n",
    "                #print('=================    ITERATION ==============', s)\n",
    "                X_new = pd.DataFrame()\n",
    "                bestX = x['fx'].idxmin(axis=1)\n",
    "                worstX = x['fx'].idxmax(axis=1)\n",
    "                #print('BEST ', bestX, 'WORST ', worstX)\n",
    "                #print('INITIAL f(x) BEST ', x['fx'].min())\n",
    "                #print('INITIAL f(x) WORST ', x['fx'].max())\n",
    "                for i in range(len(x.columns)-1):\n",
    "                    r1 = np.round(np.random.random(1),2)\n",
    "                    r2 = np.round(np.random.random(1),2)\n",
    "\n",
    "                    jaya_X = (x[x.columns[i]] + r1 * (bestX - abs(x[x.columns[i]])) - r2*(worstX - abs(x[x.columns[i]])))\n",
    "\n",
    "                    X_new[x.columns[i]] = self.binary_x_new(jaya_X)\n",
    "\n",
    "                X_new['fx'] = self.__objective_function(X_new)\n",
    "                #print(X_new)\n",
    "\n",
    "                for d in range(len(x)):\n",
    "                    remove_index = []\n",
    "                    if(X_new['fx'].iloc[d] < x['fx'].iloc[d]):\n",
    "                        x['fx'].iloc[d] = X_new['fx'].iloc[d]\n",
    "                    \n",
    "                    else:\n",
    "                                              \n",
    "                        remove_index.append(d)\n",
    "                        \n",
    "                \n",
    "                x =x.drop(index=x.index[[remove_index]])\n",
    "            else:\n",
    "                print('Only one population left')\n",
    "                break;\n",
    "                #print(x)\n",
    "        return x\n",
    "\n",
    "    def jaya_AUC(self,x):\n",
    "        '''\n",
    "        INPUT:\n",
    "        bextX is with the larger fx value, \n",
    "        worstX, \n",
    "        r1 and r2 are two random numbers between 0 and 1 \n",
    "        '''\n",
    "\n",
    "        MaxIter = 100\n",
    "        print('Processing ........')\n",
    "        for s in range(MaxIter):      # line 6\n",
    "            # print(\"# of Populations \", len(x))\n",
    "            if len(x) >1:\n",
    "               \n",
    "                #print('=================    ITERATION ==============', s)\n",
    "                X_new = pd.DataFrame()\n",
    "                bestX = x['fx'].idxmax(axis=1)\n",
    "                worstX = x['fx'].idxmin(axis=1)\n",
    "                #print('BEST ', bestX, 'WORST ', worstX)\n",
    "                #print('INITIAL f(x) BEST ', x['fx'].max())\n",
    "                #print('INITIAL f(x) WORST ', x['fx'].min())\n",
    "                for i in range(len(x.columns)-1):\n",
    "                    r1 = np.round(np.random.random(1),2)\n",
    "                    r2 = np.round(np.random.random(1),2)\n",
    "\n",
    "                    jaya_X = (x[x.columns[i]] + r1 * (bestX - abs(x[x.columns[i]])) - r2*(worstX - abs(x[x.columns[i]])))\n",
    "\n",
    "                    X_new[x.columns[i]] = self.binary_x_new(jaya_X)\n",
    "\n",
    "                X_new['fx'] = self.__objective_function(X_new)\n",
    "                #print(X_new)\n",
    "\n",
    "                for d in range(len(x)):\n",
    "                    remove_index = []\n",
    "                    if(X_new['fx'].iloc[d] > x['fx'].iloc[d]):\n",
    "\n",
    "                        x['fx'].iloc[d] = X_new['fx'].iloc[d]\n",
    "                    else:\n",
    "                                              \n",
    "                        remove_index.append(d)\n",
    "                        \n",
    "                \n",
    "                x =x.drop(index=x.index[[remove_index]])\n",
    "            else:\n",
    "                print('Only one population left')\n",
    "                break;\n",
    "                \n",
    "        return x \n",
    "    \n",
    "def error_rate(pred, orig):\n",
    "    '''\n",
    "    PURPOSE: Calculate the rate of accuracy of a classifier. \n",
    "    INPUT: pred = predicted class by a classifier orig: original class. [type binary array]. \n",
    "    OUTPUT: float error rate. \n",
    "    \n",
    "    '''\n",
    "    error = np.mean((pred != orig).astype(float))\n",
    "    return error\n",
    "\n",
    "\n",
    "## NOTE: Covert fx_ER and fx_AUC into classes, so we can transfer the classifier as well. \n",
    "'''\n",
    "class fx_function_selection:\n",
    "    def __init__(self, model_selected, function_selected):\n",
    "        \n",
    "        \n",
    "        self.function_selected = function\n",
    "'''   \n",
    "def fx_ER(X_train, x_test, Y_train, y_test):\n",
    "    # We can choose any classifier here. \n",
    "    clf = GaussianNB()\n",
    "    #clf = KNeighborsClassifier(n_neighbors=10)\n",
    "    clf.fit(X_train, np.ravel(Y_train))\n",
    "    pred_y = clf.predict(x_test)\n",
    "    return error_rate(pred_y, y_test)\n",
    "\n",
    "def fx_AUC(X_train, x_test, Y_train, y_test):\n",
    "    # We can choose any classifier here.  \n",
    "    clf = GaussianNB()\n",
    "    #clf = KNeighborsClassifier(n_neighbors=10)\n",
    "    clf.fit(X_train, np.ravel(Y_train))\n",
    "    pred_y = clf.predict_proba(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    pred_y  = pred_y [:, 1]\n",
    "    lr_auc = roc_auc_score(y_test, pred_y )\n",
    "\n",
    "    return lr_auc\n",
    "\n",
    "class binary_sets:\n",
    "    def __init__(self, D):\n",
    "        self.D = D\n",
    "    def __call__(self, population):\n",
    "        idx = (population.keys())\n",
    "        binary = np.zeros(self.D)\n",
    "        np.put(binary, idx,1)\n",
    "        return binary\n",
    "    \n",
    "        \n",
    "class feature_subset:\n",
    "    def __init__(self, list_of_features):\n",
    "        self.features = list_of_features\n",
    "        \n",
    "        \n",
    "    def __call__(self, dataframe):\n",
    "        #columns = [f  for f in self.args]\n",
    "                \n",
    "        #print(self.features)  \n",
    "        new_df = dataframe[self.features]\n",
    "        return new_df \n",
    "    \n",
    "class generate_binary_dataset:\n",
    "    def __init__(self, population, fx):\n",
    "        self.P = population\n",
    "        self.f_x = fx\n",
    "        \n",
    "        return None\n",
    "    def __call__(self,X_train, x_test, Y_train, y_test):\n",
    "        D = len(X_train.columns)\n",
    "        col_names = X_train.columns\n",
    "        binary_dsp = binary_sets(D)\n",
    "        min_features = int(D/3)\n",
    "        \n",
    "        X = {}\n",
    "        FV = []\n",
    "        \n",
    "        \n",
    "        for i in range(self.P):\n",
    "            #print('=================================  Population: ', i, \"===============================\")\n",
    "            random_D = np.arange(2,(np.random.randint(min_features, D)))\n",
    "            #print(i, '# of features: ', len(random_D))\n",
    "            feature_sub = feature_subset(random_D)\n",
    "            \n",
    "            tem_train_X = feature_sub(X_train)\n",
    "            tem_test_X  = feature_sub(x_test)\n",
    "            \n",
    "            \n",
    "                       \n",
    "            FV.append(self.f_x(X_train = tem_train_X,\n",
    "                          x_test  = tem_test_X,\n",
    "                          Y_train = Y_train,\n",
    "                          y_test  = y_test)\n",
    "                     )\n",
    "            \n",
    "            \n",
    "            X[i] =  binary_dsp(tem_test_X)\n",
    "        Xx = pd.DataFrame(X.values(), columns = col_names ,index = X.keys())\n",
    "        \n",
    "        Xx['fx'] = FV\n",
    "        \n",
    "        #print(Xx.head(10))\n",
    "        return Xx\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING PROPOSED STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      " -----------RESULT # -------  0\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  72\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7081    0.8476    0.7716       269\n",
      "         1.0     0.7338    0.5459    0.6260       207\n",
      "\n",
      "    accuracy                         0.7164       476\n",
      "   macro avg     0.7209    0.6967    0.6988       476\n",
      "weighted avg     0.7192    0.7164    0.7083       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8271    0.8178    0.8224       269\n",
      "         1.0     0.7667    0.7778    0.7722       207\n",
      "\n",
      "    accuracy                         0.8004       476\n",
      "   macro avg     0.7969    0.7978    0.7973       476\n",
      "weighted avg     0.8008    0.8004    0.8006       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8456    0.9368    0.8889       269\n",
      "         1.0     0.9045    0.7778    0.8364       207\n",
      "\n",
      "    accuracy                         0.8676       476\n",
      "   macro avg     0.8751    0.8573    0.8626       476\n",
      "weighted avg     0.8712    0.8676    0.8660       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7148    0.7918    0.7513       269\n",
      "         1.0     0.6854    0.5894    0.6338       207\n",
      "\n",
      "    accuracy                         0.7038       476\n",
      "   macro avg     0.7001    0.6906    0.6925       476\n",
      "weighted avg     0.7020    0.7038    0.7002       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  1\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  111\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7013    0.8290    0.7598       269\n",
      "         1.0     0.7089    0.5411    0.6137       207\n",
      "\n",
      "    accuracy                         0.7038       476\n",
      "   macro avg     0.7051    0.6850    0.6867       476\n",
      "weighted avg     0.7046    0.7038    0.6963       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8449    0.7695    0.8054       269\n",
      "         1.0     0.7316    0.8164    0.7717       207\n",
      "\n",
      "    accuracy                         0.7899       476\n",
      "   macro avg     0.7882    0.7930    0.7886       476\n",
      "weighted avg     0.7956    0.7899    0.7908       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8562    0.9294    0.8913       269\n",
      "         1.0     0.8967    0.7971    0.8440       207\n",
      "\n",
      "    accuracy                         0.8718       476\n",
      "   macro avg     0.8765    0.8632    0.8676       476\n",
      "weighted avg     0.8738    0.8718    0.8707       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7466    0.8104    0.7772       269\n",
      "         1.0     0.7228    0.6425    0.6803       207\n",
      "\n",
      "    accuracy                         0.7374       476\n",
      "   macro avg     0.7347    0.7265    0.7287       476\n",
      "weighted avg     0.7362    0.7374    0.7351       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  2\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  117\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7079    0.8290    0.7637       269\n",
      "         1.0     0.7143    0.5556    0.6250       207\n",
      "\n",
      "    accuracy                         0.7101       476\n",
      "   macro avg     0.7111    0.6923    0.6943       476\n",
      "weighted avg     0.7107    0.7101    0.7034       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8340    0.7844    0.8084       269\n",
      "         1.0     0.7399    0.7971    0.7674       207\n",
      "\n",
      "    accuracy                         0.7899       476\n",
      "   macro avg     0.7870    0.7907    0.7879       476\n",
      "weighted avg     0.7931    0.7899    0.7906       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8720    0.9368    0.9032       269\n",
      "         1.0     0.9091    0.8213    0.8629       207\n",
      "\n",
      "    accuracy                         0.8866       476\n",
      "   macro avg     0.8905    0.8790    0.8831       476\n",
      "weighted avg     0.8881    0.8866    0.8857       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7641    0.8067    0.7848       269\n",
      "         1.0     0.7292    0.6763    0.7018       207\n",
      "\n",
      "    accuracy                         0.7500       476\n",
      "   macro avg     0.7466    0.7415    0.7433       476\n",
      "weighted avg     0.7489    0.7500    0.7487       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  3\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  130\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7037    0.8476    0.7690       269\n",
      "         1.0     0.7303    0.5362    0.6184       207\n",
      "\n",
      "    accuracy                         0.7122       476\n",
      "   macro avg     0.7170    0.6919    0.6937       476\n",
      "weighted avg     0.7153    0.7122    0.7035       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7814    0.8104    0.7956       269\n",
      "         1.0     0.7411    0.7053    0.7228       207\n",
      "\n",
      "    accuracy                         0.7647       476\n",
      "   macro avg     0.7612    0.7579    0.7592       476\n",
      "weighted avg     0.7639    0.7647    0.7639       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8664    0.9405    0.9020       269\n",
      "         1.0     0.9130    0.8116    0.8593       207\n",
      "\n",
      "    accuracy                         0.8845       476\n",
      "   macro avg     0.8897    0.8761    0.8806       476\n",
      "weighted avg     0.8867    0.8845    0.8834       476\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7616    0.8550    0.8056       269\n",
      "         1.0     0.7759    0.6522    0.7087       207\n",
      "\n",
      "    accuracy                         0.7668       476\n",
      "   macro avg     0.7687    0.7536    0.7571       476\n",
      "weighted avg     0.7678    0.7668    0.7634       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  4\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  85\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7085    0.8401    0.7687       269\n",
      "         1.0     0.7261    0.5507    0.6264       207\n",
      "\n",
      "    accuracy                         0.7143       476\n",
      "   macro avg     0.7173    0.6954    0.6975       476\n",
      "weighted avg     0.7161    0.7143    0.7068       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8268    0.7807    0.8031       269\n",
      "         1.0     0.7342    0.7874    0.7599       207\n",
      "\n",
      "    accuracy                         0.7836       476\n",
      "   macro avg     0.7805    0.7841    0.7815       476\n",
      "weighted avg     0.7865    0.7836    0.7843       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8451    0.9331    0.8869       269\n",
      "         1.0     0.8994    0.7778    0.8342       207\n",
      "\n",
      "    accuracy                         0.8655       476\n",
      "   macro avg     0.8723    0.8554    0.8606       476\n",
      "weighted avg     0.8687    0.8655    0.8640       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7216    0.7807    0.7500       269\n",
      "         1.0     0.6811    0.6087    0.6429       207\n",
      "\n",
      "    accuracy                         0.7059       476\n",
      "   macro avg     0.7014    0.6947    0.6964       476\n",
      "weighted avg     0.7040    0.7059    0.7034       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  5\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  76\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7138    0.8439    0.7734       269\n",
      "         1.0     0.7342    0.5604    0.6356       207\n",
      "\n",
      "    accuracy                         0.7206       476\n",
      "   macro avg     0.7240    0.7021    0.7045       476\n",
      "weighted avg     0.7227    0.7206    0.7135       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8242    0.8364    0.8303       269\n",
      "         1.0     0.7833    0.7681    0.7756       207\n",
      "\n",
      "    accuracy                         0.8067       476\n",
      "   macro avg     0.8037    0.8023    0.8029       476\n",
      "weighted avg     0.8064    0.8067    0.8065       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8514    0.9368    0.8920       269\n",
      "         1.0     0.9056    0.7874    0.8424       207\n",
      "\n",
      "    accuracy                         0.8718       476\n",
      "   macro avg     0.8785    0.8621    0.8672       476\n",
      "weighted avg     0.8749    0.8718    0.8704       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7205    0.7955    0.7562       269\n",
      "         1.0     0.6927    0.5990    0.6425       207\n",
      "\n",
      "    accuracy                         0.7101       476\n",
      "   macro avg     0.7066    0.6973    0.6993       476\n",
      "weighted avg     0.7084    0.7101    0.7067       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  6\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  100\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7217    0.8290    0.7716       269\n",
      "         1.0     0.7246    0.5845    0.6471       207\n",
      "\n",
      "    accuracy                         0.7227       476\n",
      "   macro avg     0.7231    0.7068    0.7093       476\n",
      "weighted avg     0.7229    0.7227    0.7175       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7754    0.8216    0.7978       269\n",
      "         1.0     0.7487    0.6908    0.7186       207\n",
      "\n",
      "    accuracy                         0.7647       476\n",
      "   macro avg     0.7621    0.7562    0.7582       476\n",
      "weighted avg     0.7638    0.7647    0.7634       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8741    0.9294    0.9009       269\n",
      "         1.0     0.9000    0.8261    0.8615       207\n",
      "\n",
      "    accuracy                         0.8845       476\n",
      "   macro avg     0.8871    0.8777    0.8812       476\n",
      "weighted avg     0.8854    0.8845    0.8837       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7415    0.8104    0.7744       269\n",
      "         1.0     0.7198    0.6329    0.6735       207\n",
      "\n",
      "    accuracy                         0.7332       476\n",
      "   macro avg     0.7306    0.7216    0.7240       476\n",
      "weighted avg     0.7321    0.7332    0.7305       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  7\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  78\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6985    0.8439    0.7643       269\n",
      "         1.0     0.7219    0.5266    0.6089       207\n",
      "\n",
      "    accuracy                         0.7059       476\n",
      "   macro avg     0.7102    0.6852    0.6866       476\n",
      "weighted avg     0.7086    0.7059    0.6967       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8435    0.8216    0.8324       269\n",
      "         1.0     0.7757    0.8019    0.7886       207\n",
      "\n",
      "    accuracy                         0.8130       476\n",
      "   macro avg     0.8096    0.8117    0.8105       476\n",
      "weighted avg     0.8140    0.8130    0.8133       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8433    0.9405    0.8893       269\n",
      "         1.0     0.9091    0.7729    0.8355       207\n",
      "\n",
      "    accuracy                         0.8676       476\n",
      "   macro avg     0.8762    0.8567    0.8624       476\n",
      "weighted avg     0.8719    0.8676    0.8659       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7191    0.7993    0.7570       269\n",
      "         1.0     0.6949    0.5942    0.6406       207\n",
      "\n",
      "    accuracy                         0.7101       476\n",
      "   macro avg     0.7070    0.6967    0.6988       476\n",
      "weighted avg     0.7086    0.7101    0.7064       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  8\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  122\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7009    0.8364    0.7627       269\n",
      "         1.0     0.7161    0.5362    0.6133       207\n",
      "\n",
      "    accuracy                         0.7059       476\n",
      "   macro avg     0.7085    0.6863    0.6880       476\n",
      "weighted avg     0.7075    0.7059    0.6977       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7606    0.8030    0.7812       269\n",
      "         1.0     0.7240    0.6715    0.6967       207\n",
      "\n",
      "    accuracy                         0.7458       476\n",
      "   macro avg     0.7423    0.7372    0.7390       476\n",
      "weighted avg     0.7446    0.7458    0.7445       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8750    0.9368    0.9048       269\n",
      "         1.0     0.9096    0.8261    0.8658       207\n",
      "\n",
      "    accuracy                         0.8887       476\n",
      "   macro avg     0.8923    0.8814    0.8853       476\n",
      "weighted avg     0.8900    0.8887    0.8879       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7855    0.8439    0.8136       269\n",
      "         1.0     0.7754    0.7005    0.7360       207\n",
      "\n",
      "    accuracy                         0.7815       476\n",
      "   macro avg     0.7804    0.7722    0.7748       476\n",
      "weighted avg     0.7811    0.7815    0.7799       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  9\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  152\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7147    0.8476    0.7755       269\n",
      "         1.0     0.7389    0.5604    0.6374       207\n",
      "\n",
      "    accuracy                         0.7227       476\n",
      "   macro avg     0.7268    0.7040    0.7064       476\n",
      "weighted avg     0.7252    0.7227    0.7154       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7621    0.7621    0.7621       269\n",
      "         1.0     0.6908    0.6908    0.6908       207\n",
      "\n",
      "    accuracy                         0.7311       476\n",
      "   macro avg     0.7265    0.7265    0.7265       476\n",
      "weighted avg     0.7311    0.7311    0.7311       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8601    0.9368    0.8968       269\n",
      "         1.0     0.9071    0.8019    0.8513       207\n",
      "\n",
      "    accuracy                         0.8782       476\n",
      "   macro avg     0.8836    0.8694    0.8740       476\n",
      "weighted avg     0.8805    0.8782    0.8770       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7607    0.8625    0.8084       269\n",
      "         1.0     0.7836    0.6473    0.7090       207\n",
      "\n",
      "    accuracy                         0.7689       476\n",
      "   macro avg     0.7721    0.7549    0.7587       476\n",
      "weighted avg     0.7706    0.7689    0.7651       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in range(10):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = feature_optimization(trainX = Musk_X,\n",
    "                               trainY = Musk_Y,\n",
    "                               testX = Musk_x,\n",
    "                               testY = Musk_y, \n",
    "                               population = 10,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MuskX_OP, Muskx_OP = dsp()\n",
    "    print('Number of Features ', MuskX_OP.shape[1])\n",
    "    musk_return = data_classification(train_x = MuskX_OP, train_y = Musk_Y, \n",
    "                                  test_x = Muskx_OP ,test_y = Musk_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      " -----------RESULT # -------  0\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  417\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5035    0.4778    0.4903       900\n",
      "         1.0     0.5032    0.5289    0.5157       900\n",
      "\n",
      "    accuracy                         0.5033      1800\n",
      "   macro avg     0.5033    0.5033    0.5030      1800\n",
      "weighted avg     0.5033    0.5033    0.5030      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4951    0.5022    0.4986       900\n",
      "         1.0     0.4949    0.4878    0.4913       900\n",
      "\n",
      "    accuracy                         0.4950      1800\n",
      "   macro avg     0.4950    0.4950    0.4950      1800\n",
      "weighted avg     0.4950    0.4950    0.4950      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4974    0.6356    0.5580       900\n",
      "         1.0     0.4954    0.3578    0.4155       900\n",
      "\n",
      "    accuracy                         0.4967      1800\n",
      "   macro avg     0.4964    0.4967    0.4868      1800\n",
      "weighted avg     0.4964    0.4967    0.4868      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4971    0.4822    0.4896       900\n",
      "         1.0     0.4973    0.5122    0.5047       900\n",
      "\n",
      "    accuracy                         0.4972      1800\n",
      "   macro avg     0.4972    0.4972    0.4971      1800\n",
      "weighted avg     0.4972    0.4972    0.4971      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  1\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  244\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5017    0.4789    0.4901       900\n",
      "         1.0     0.5016    0.5244    0.5128       900\n",
      "\n",
      "    accuracy                         0.5017      1800\n",
      "   macro avg     0.5017    0.5017    0.5014      1800\n",
      "weighted avg     0.5017    0.5017    0.5014      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5017    0.4911    0.4964       900\n",
      "         1.0     0.5016    0.5122    0.5069       900\n",
      "\n",
      "    accuracy                         0.5017      1800\n",
      "   macro avg     0.5017    0.5017    0.5016      1800\n",
      "weighted avg     0.5017    0.5017    0.5016      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4882    0.6000    0.5384       900\n",
      "         1.0     0.4813    0.3711    0.4191       900\n",
      "\n",
      "    accuracy                         0.4856      1800\n",
      "   macro avg     0.4848    0.4856    0.4787      1800\n",
      "weighted avg     0.4848    0.4856    0.4787      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5035    0.4800    0.4915       900\n",
      "         1.0     0.5032    0.5267    0.5147       900\n",
      "\n",
      "    accuracy                         0.5033      1800\n",
      "   macro avg     0.5033    0.5033    0.5031      1800\n",
      "weighted avg     0.5033    0.5033    0.5031      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  2\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  461\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5047    0.4778    0.4909       900\n",
      "         1.0     0.5042    0.5311    0.5173       900\n",
      "\n",
      "    accuracy                         0.5044      1800\n",
      "   macro avg     0.5045    0.5044    0.5041      1800\n",
      "weighted avg     0.5045    0.5044    0.5041      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4852    0.4744    0.4798       900\n",
      "         1.0     0.4859    0.4967    0.4912       900\n",
      "\n",
      "    accuracy                         0.4856      1800\n",
      "   macro avg     0.4855    0.4856    0.4855      1800\n",
      "weighted avg     0.4855    0.4856    0.4855      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4970    0.6400    0.5595       900\n",
      "         1.0     0.4945    0.3522    0.4114       900\n",
      "\n",
      "    accuracy                         0.4961      1800\n",
      "   macro avg     0.4958    0.4961    0.4855      1800\n",
      "weighted avg     0.4958    0.4961    0.4855      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5115    0.4933    0.5023       900\n",
      "         1.0     0.5107    0.5289    0.5197       900\n",
      "\n",
      "    accuracy                         0.5111      1800\n",
      "   macro avg     0.5111    0.5111    0.5110      1800\n",
      "weighted avg     0.5111    0.5111    0.5110      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  3\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  442\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4959    0.4744    0.4850       900\n",
      "         1.0     0.4963    0.5178    0.5068       900\n",
      "\n",
      "    accuracy                         0.4961      1800\n",
      "   macro avg     0.4961    0.4961    0.4959      1800\n",
      "weighted avg     0.4961    0.4961    0.4959      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4849    0.5167    0.5003       900\n",
      "         1.0     0.4828    0.4511    0.4664       900\n",
      "\n",
      "    accuracy                         0.4839      1800\n",
      "   macro avg     0.4838    0.4839    0.4833      1800\n",
      "weighted avg     0.4838    0.4839    0.4833      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4978    0.6389    0.5596       900\n",
      "         1.0     0.4961    0.3556    0.4142       900\n",
      "\n",
      "    accuracy                         0.4972      1800\n",
      "   macro avg     0.4970    0.4972    0.4869      1800\n",
      "weighted avg     0.4970    0.4972    0.4869      1800\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5017    0.4867    0.4941       900\n",
      "         1.0     0.5016    0.5167    0.5090       900\n",
      "\n",
      "    accuracy                         0.5017      1800\n",
      "   macro avg     0.5017    0.5017    0.5016      1800\n",
      "weighted avg     0.5017    0.5017    0.5016      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  4\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  235\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5000    0.4778    0.4886       900\n",
      "         1.0     0.5000    0.5222    0.5109       900\n",
      "\n",
      "    accuracy                         0.5000      1800\n",
      "   macro avg     0.5000    0.5000    0.4998      1800\n",
      "weighted avg     0.5000    0.5000    0.4998      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5000    0.5133    0.5066       900\n",
      "         1.0     0.5000    0.4867    0.4932       900\n",
      "\n",
      "    accuracy                         0.5000      1800\n",
      "   macro avg     0.5000    0.5000    0.4999      1800\n",
      "weighted avg     0.5000    0.5000    0.4999      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5027    0.6311    0.5596       900\n",
      "         1.0     0.5045    0.3756    0.4306       900\n",
      "\n",
      "    accuracy                         0.5033      1800\n",
      "   macro avg     0.5036    0.5033    0.4951      1800\n",
      "weighted avg     0.5036    0.5033    0.4951      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5086    0.4956    0.5020       900\n",
      "         1.0     0.5081    0.5211    0.5145       900\n",
      "\n",
      "    accuracy                         0.5083      1800\n",
      "   macro avg     0.5083    0.5083    0.5083      1800\n",
      "weighted avg     0.5083    0.5083    0.5083      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  5\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  290\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4971    0.4722    0.4843       900\n",
      "         1.0     0.4974    0.5222    0.5095       900\n",
      "\n",
      "    accuracy                         0.4972      1800\n",
      "   macro avg     0.4972    0.4972    0.4969      1800\n",
      "weighted avg     0.4972    0.4972    0.4969      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4944    0.4889    0.4916       900\n",
      "         1.0     0.4945    0.5000    0.4972       900\n",
      "\n",
      "    accuracy                         0.4944      1800\n",
      "   macro avg     0.4944    0.4944    0.4944      1800\n",
      "weighted avg     0.4944    0.4944    0.4944      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4863    0.6100    0.5412       900\n",
      "         1.0     0.4769    0.3556    0.4074       900\n",
      "\n",
      "    accuracy                         0.4828      1800\n",
      "   macro avg     0.4816    0.4828    0.4743      1800\n",
      "weighted avg     0.4816    0.4828    0.4743      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5092    0.4911    0.5000       900\n",
      "         1.0     0.5086    0.5267    0.5175       900\n",
      "\n",
      "    accuracy                         0.5089      1800\n",
      "   macro avg     0.5089    0.5089    0.5087      1800\n",
      "weighted avg     0.5089    0.5089    0.5087      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  6\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  312\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4988    0.4711    0.4846       900\n",
      "         1.0     0.4989    0.5267    0.5124       900\n",
      "\n",
      "    accuracy                         0.4989      1800\n",
      "   macro avg     0.4989    0.4989    0.4985      1800\n",
      "weighted avg     0.4989    0.4989    0.4985      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4881    0.5022    0.4951       900\n",
      "         1.0     0.4874    0.4733    0.4803       900\n",
      "\n",
      "    accuracy                         0.4878      1800\n",
      "   macro avg     0.4878    0.4878    0.4877      1800\n",
      "weighted avg     0.4878    0.4878    0.4877      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4931    0.5967    0.5400       900\n",
      "         1.0     0.4895    0.3867    0.4320       900\n",
      "\n",
      "    accuracy                         0.4917      1800\n",
      "   macro avg     0.4913    0.4917    0.4860      1800\n",
      "weighted avg     0.4913    0.4917    0.4860      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5151    0.4922    0.5034       900\n",
      "         1.0     0.5138    0.5367    0.5250       900\n",
      "\n",
      "    accuracy                         0.5144      1800\n",
      "   macro avg     0.5145    0.5144    0.5142      1800\n",
      "weighted avg     0.5145    0.5144    0.5142      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  7\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  241\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5012    0.4689    0.4845       900\n",
      "         1.0     0.5010    0.5333    0.5167       900\n",
      "\n",
      "    accuracy                         0.5011      1800\n",
      "   macro avg     0.5011    0.5011    0.5006      1800\n",
      "weighted avg     0.5011    0.5011    0.5006      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5045    0.4978    0.5011       900\n",
      "         1.0     0.5044    0.5111    0.5077       900\n",
      "\n",
      "    accuracy                         0.5044      1800\n",
      "   macro avg     0.5044    0.5044    0.5044      1800\n",
      "weighted avg     0.5044    0.5044    0.5044      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4897    0.6056    0.5415       900\n",
      "         1.0     0.4833    0.3689    0.4184       900\n",
      "\n",
      "    accuracy                         0.4872      1800\n",
      "   macro avg     0.4865    0.4872    0.4799      1800\n",
      "weighted avg     0.4865    0.4872    0.4799      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4994    0.4756    0.4872       900\n",
      "         1.0     0.4995    0.5233    0.5111       900\n",
      "\n",
      "    accuracy                         0.4994      1800\n",
      "   macro avg     0.4994    0.4994    0.4992      1800\n",
      "weighted avg     0.4994    0.4994    0.4992      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  8\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  255\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4947    0.4711    0.4826       900\n",
      "         1.0     0.4952    0.5189    0.5068       900\n",
      "\n",
      "    accuracy                         0.4950      1800\n",
      "   macro avg     0.4950    0.4950    0.4947      1800\n",
      "weighted avg     0.4950    0.4950    0.4947      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4977    0.4900    0.4938       900\n",
      "         1.0     0.4978    0.5056    0.5017       900\n",
      "\n",
      "    accuracy                         0.4978      1800\n",
      "   macro avg     0.4978    0.4978    0.4977      1800\n",
      "weighted avg     0.4978    0.4978    0.4977      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5009    0.6089    0.5496       900\n",
      "         1.0     0.5014    0.3933    0.4408       900\n",
      "\n",
      "    accuracy                         0.5011      1800\n",
      "   macro avg     0.5012    0.5011    0.4952      1800\n",
      "weighted avg     0.5012    0.5011    0.4952      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5045    0.4933    0.4989       900\n",
      "         1.0     0.5043    0.5156    0.5099       900\n",
      "\n",
      "    accuracy                         0.5044      1800\n",
      "   macro avg     0.5044    0.5044    0.5044      1800\n",
      "weighted avg     0.5044    0.5044    0.5044      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  9\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  357\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5012    0.4800    0.4904       900\n",
      "         1.0     0.5011    0.5222    0.5114       900\n",
      "\n",
      "    accuracy                         0.5011      1800\n",
      "   macro avg     0.5011    0.5011    0.5009      1800\n",
      "weighted avg     0.5011    0.5011    0.5009      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5000    0.5089    0.5044       900\n",
      "         1.0     0.5000    0.4911    0.4955       900\n",
      "\n",
      "    accuracy                         0.5000      1800\n",
      "   macro avg     0.5000    0.5000    0.5000      1800\n",
      "weighted avg     0.5000    0.5000    0.5000      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4956    0.6189    0.5504       900\n",
      "         1.0     0.4926    0.3700    0.4226       900\n",
      "\n",
      "    accuracy                         0.4944      1800\n",
      "   macro avg     0.4941    0.4944    0.4865      1800\n",
      "weighted avg     0.4941    0.4944    0.4865      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5023    0.4811    0.4915       900\n",
      "         1.0     0.5021    0.5233    0.5125       900\n",
      "\n",
      "    accuracy                         0.5022      1800\n",
      "   macro avg     0.5022    0.5022    0.5020      1800\n",
      "weighted avg     0.5022    0.5022    0.5020      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  10\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  176\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5006    0.4822    0.4912       900\n",
      "         1.0     0.5005    0.5189    0.5095       900\n",
      "\n",
      "    accuracy                         0.5006      1800\n",
      "   macro avg     0.5006    0.5006    0.5004      1800\n",
      "weighted avg     0.5006    0.5006    0.5004      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5084    0.5356    0.5216       900\n",
      "         1.0     0.5094    0.4822    0.4954       900\n",
      "\n",
      "    accuracy                         0.5089      1800\n",
      "   macro avg     0.5089    0.5089    0.5085      1800\n",
      "weighted avg     0.5089    0.5089    0.5085      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4990    0.5556    0.5258       900\n",
      "         1.0     0.4987    0.4422    0.4688       900\n",
      "\n",
      "    accuracy                         0.4989      1800\n",
      "   macro avg     0.4989    0.4989    0.4973      1800\n",
      "weighted avg     0.4989    0.4989    0.4973      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5029    0.4800    0.4912       900\n",
      "         1.0     0.5027    0.5256    0.5139       900\n",
      "\n",
      "    accuracy                         0.5028      1800\n",
      "   macro avg     0.5028    0.5028    0.5025      1800\n",
      "weighted avg     0.5028    0.5028    0.5025      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  11\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  290\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4971    0.4722    0.4843       900\n",
      "         1.0     0.4974    0.5222    0.5095       900\n",
      "\n",
      "    accuracy                         0.4972      1800\n",
      "   macro avg     0.4972    0.4972    0.4969      1800\n",
      "weighted avg     0.4972    0.4972    0.4969      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5038    0.5100    0.5069       900\n",
      "         1.0     0.5039    0.4978    0.5008       900\n",
      "\n",
      "    accuracy                         0.5039      1800\n",
      "   macro avg     0.5039    0.5039    0.5039      1800\n",
      "weighted avg     0.5039    0.5039    0.5039      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4863    0.6100    0.5412       900\n",
      "         1.0     0.4769    0.3556    0.4074       900\n",
      "\n",
      "    accuracy                         0.4828      1800\n",
      "   macro avg     0.4816    0.4828    0.4743      1800\n",
      "weighted avg     0.4816    0.4828    0.4743      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5092    0.4911    0.5000       900\n",
      "         1.0     0.5086    0.5267    0.5175       900\n",
      "\n",
      "    accuracy                         0.5089      1800\n",
      "   macro avg     0.5089    0.5089    0.5087      1800\n",
      "weighted avg     0.5089    0.5089    0.5087      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  12\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  406\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5017    0.4789    0.4901       900\n",
      "         1.0     0.5016    0.5244    0.5128       900\n",
      "\n",
      "    accuracy                         0.5017      1800\n",
      "   macro avg     0.5017    0.5017    0.5014      1800\n",
      "weighted avg     0.5017    0.5017    0.5014      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4821    0.4933    0.4876       900\n",
      "         1.0     0.4812    0.4700    0.4755       900\n",
      "\n",
      "    accuracy                         0.4817      1800\n",
      "   macro avg     0.4817    0.4817    0.4816      1800\n",
      "weighted avg     0.4817    0.4817    0.4816      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4986    0.6144    0.5505       900\n",
      "         1.0     0.4978    0.3822    0.4324       900\n",
      "\n",
      "    accuracy                         0.4983      1800\n",
      "   macro avg     0.4982    0.4983    0.4915      1800\n",
      "weighted avg     0.4982    0.4983    0.4915      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5023    0.4789    0.4903       900\n",
      "         1.0     0.5021    0.5256    0.5136       900\n",
      "\n",
      "    accuracy                         0.5022      1800\n",
      "   macro avg     0.5022    0.5022    0.5020      1800\n",
      "weighted avg     0.5022    0.5022    0.5020      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  13\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  331\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4936    0.4689    0.4809       900\n",
      "         1.0     0.4942    0.5189    0.5062       900\n",
      "\n",
      "    accuracy                         0.4939      1800\n",
      "   macro avg     0.4939    0.4939    0.4936      1800\n",
      "weighted avg     0.4939    0.4939    0.4936      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4950    0.5000    0.4975       900\n",
      "         1.0     0.4949    0.4900    0.4925       900\n",
      "\n",
      "    accuracy                         0.4950      1800\n",
      "   macro avg     0.4950    0.4950    0.4950      1800\n",
      "weighted avg     0.4950    0.4950    0.4950      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5112    0.6078    0.5553       900\n",
      "         1.0     0.5164    0.4189    0.4626       900\n",
      "\n",
      "    accuracy                         0.5133      1800\n",
      "   macro avg     0.5138    0.5133    0.5090      1800\n",
      "weighted avg     0.5138    0.5133    0.5090      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5126    0.4956    0.5040       900\n",
      "         1.0     0.5118    0.5289    0.5202       900\n",
      "\n",
      "    accuracy                         0.5122      1800\n",
      "   macro avg     0.5122    0.5122    0.5121      1800\n",
      "weighted avg     0.5122    0.5122    0.5121      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  14\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  461\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5047    0.4778    0.4909       900\n",
      "         1.0     0.5042    0.5311    0.5173       900\n",
      "\n",
      "    accuracy                         0.5044      1800\n",
      "   macro avg     0.5045    0.5044    0.5041      1800\n",
      "weighted avg     0.5045    0.5044    0.5041      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4789    0.4800    0.4795       900\n",
      "         1.0     0.4788    0.4778    0.4783       900\n",
      "\n",
      "    accuracy                         0.4789      1800\n",
      "   macro avg     0.4789    0.4789    0.4789      1800\n",
      "weighted avg     0.4789    0.4789    0.4789      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4970    0.6400    0.5595       900\n",
      "         1.0     0.4945    0.3522    0.4114       900\n",
      "\n",
      "    accuracy                         0.4961      1800\n",
      "   macro avg     0.4958    0.4961    0.4855      1800\n",
      "weighted avg     0.4958    0.4961    0.4855      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5115    0.4933    0.5023       900\n",
      "         1.0     0.5107    0.5289    0.5197       900\n",
      "\n",
      "    accuracy                         0.5111      1800\n",
      "   macro avg     0.5111    0.5111    0.5110      1800\n",
      "weighted avg     0.5111    0.5111    0.5110      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  15\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  248\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4988    0.4722    0.4852       900\n",
      "         1.0     0.4989    0.5256    0.5119       900\n",
      "\n",
      "    accuracy                         0.4989      1800\n",
      "   macro avg     0.4989    0.4989    0.4985      1800\n",
      "weighted avg     0.4989    0.4989    0.4985      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4905    0.4900    0.4903       900\n",
      "         1.0     0.4906    0.4911    0.4908       900\n",
      "\n",
      "    accuracy                         0.4906      1800\n",
      "   macro avg     0.4906    0.4906    0.4906      1800\n",
      "weighted avg     0.4906    0.4906    0.4906      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4898    0.6133    0.5446       900\n",
      "         1.0     0.4829    0.3611    0.4132       900\n",
      "\n",
      "    accuracy                         0.4872      1800\n",
      "   macro avg     0.4864    0.4872    0.4789      1800\n",
      "weighted avg     0.4864    0.4872    0.4789      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5046    0.4867    0.4955       900\n",
      "         1.0     0.5043    0.5222    0.5131       900\n",
      "\n",
      "    accuracy                         0.5044      1800\n",
      "   macro avg     0.5045    0.5044    0.5043      1800\n",
      "weighted avg     0.5045    0.5044    0.5043      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  16\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  269\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4988    0.4756    0.4869       900\n",
      "         1.0     0.4989    0.5222    0.5103       900\n",
      "\n",
      "    accuracy                         0.4989      1800\n",
      "   macro avg     0.4989    0.4989    0.4986      1800\n",
      "weighted avg     0.4989    0.4989    0.4986      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5022    0.4978    0.5000       900\n",
      "         1.0     0.5022    0.5067    0.5044       900\n",
      "\n",
      "    accuracy                         0.5022      1800\n",
      "   macro avg     0.5022    0.5022    0.5022      1800\n",
      "weighted avg     0.5022    0.5022    0.5022      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4971    0.5811    0.5359       900\n",
      "         1.0     0.4960    0.4122    0.4502       900\n",
      "\n",
      "    accuracy                         0.4967      1800\n",
      "   macro avg     0.4966    0.4967    0.4931      1800\n",
      "weighted avg     0.4966    0.4967    0.4931      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5034    0.4978    0.5006       900\n",
      "         1.0     0.5033    0.5089    0.5061       900\n",
      "\n",
      "    accuracy                         0.5033      1800\n",
      "   macro avg     0.5033    0.5033    0.5033      1800\n",
      "weighted avg     0.5033    0.5033    0.5033      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  17\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  362\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5088    0.4844    0.4963       900\n",
      "         1.0     0.5080    0.5322    0.5198       900\n",
      "\n",
      "    accuracy                         0.5083      1800\n",
      "   macro avg     0.5084    0.5083    0.5081      1800\n",
      "weighted avg     0.5084    0.5083    0.5081      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5022    0.4989    0.5006       900\n",
      "         1.0     0.5022    0.5056    0.5039       900\n",
      "\n",
      "    accuracy                         0.5022      1800\n",
      "   macro avg     0.5022    0.5022    0.5022      1800\n",
      "weighted avg     0.5022    0.5022    0.5022      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4932    0.6033    0.5427       900\n",
      "         1.0     0.4893    0.3800    0.4278       900\n",
      "\n",
      "    accuracy                         0.4917      1800\n",
      "   macro avg     0.4912    0.4917    0.4852      1800\n",
      "weighted avg     0.4912    0.4917    0.4852      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5012    0.4800    0.4904       900\n",
      "         1.0     0.5011    0.5222    0.5114       900\n",
      "\n",
      "    accuracy                         0.5011      1800\n",
      "   macro avg     0.5011    0.5011    0.5009      1800\n",
      "weighted avg     0.5011    0.5011    0.5009      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  18\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  258\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4902    0.4700    0.4799       900\n",
      "         1.0     0.4909    0.5111    0.5008       900\n",
      "\n",
      "    accuracy                         0.4906      1800\n",
      "   macro avg     0.4905    0.4906    0.4903      1800\n",
      "weighted avg     0.4905    0.4906    0.4903      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4989    0.4922    0.4955       900\n",
      "         1.0     0.4989    0.5056    0.5022       900\n",
      "\n",
      "    accuracy                         0.4989      1800\n",
      "   macro avg     0.4989    0.4989    0.4989      1800\n",
      "weighted avg     0.4989    0.4989    0.4989      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4913    0.5978    0.5393       900\n",
      "         1.0     0.4865    0.3811    0.4274       900\n",
      "\n",
      "    accuracy                         0.4894      1800\n",
      "   macro avg     0.4889    0.4894    0.4834      1800\n",
      "weighted avg     0.4889    0.4894    0.4834      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4994    0.4922    0.4958       900\n",
      "         1.0     0.4995    0.5067    0.5030       900\n",
      "\n",
      "    accuracy                         0.4994      1800\n",
      "   macro avg     0.4994    0.4994    0.4994      1800\n",
      "weighted avg     0.4994    0.4994    0.4994      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  19\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  182\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5006    0.4789    0.4895       900\n",
      "         1.0     0.5005    0.5222    0.5111       900\n",
      "\n",
      "    accuracy                         0.5006      1800\n",
      "   macro avg     0.5006    0.5006    0.5003      1800\n",
      "weighted avg     0.5006    0.5006    0.5003      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5055    0.5122    0.5088       900\n",
      "         1.0     0.5056    0.4989    0.5022       900\n",
      "\n",
      "    accuracy                         0.5056      1800\n",
      "   macro avg     0.5056    0.5056    0.5055      1800\n",
      "weighted avg     0.5056    0.5056    0.5055      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5090    0.5644    0.5353       900\n",
      "         1.0     0.5112    0.4556    0.4818       900\n",
      "\n",
      "    accuracy                         0.5100      1800\n",
      "   macro avg     0.5101    0.5100    0.5085      1800\n",
      "weighted avg     0.5101    0.5100    0.5085      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5017    0.4800    0.4906       900\n",
      "         1.0     0.5016    0.5233    0.5122       900\n",
      "\n",
      "    accuracy                         0.5017      1800\n",
      "   macro avg     0.5017    0.5017    0.5014      1800\n",
      "weighted avg     0.5017    0.5017    0.5014      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in range(20):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = feature_optimization(trainX = Madelon_X,\n",
    "                               trainY = Madelon_Y,\n",
    "                               testX = Madelon_x,\n",
    "                               testY = Madelon_y, \n",
    "                               population = 30,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MadelonX_OP, Madelonx_OP = dsp()\n",
    "    print('Number of Features ', MadelonX_OP.shape[1])\n",
    "    Madelon_return = data_classification(train_x = MadelonX_OP, train_y = Madelon_Y, \n",
    "                                         test_x = Madelonx_OP ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
