{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose \n",
    "### Feature Selection by optimizing FSJaya Algorithm. \n",
    "\n",
    "The challenge is to tweak the present algorithm for feature selection purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_function\n",
      "function not available\n",
      "hyperbolic_function\n",
      "logistic_function\n",
      "logistic_function\n",
      "function not available\n",
      "hyperbolic_function\n",
      "function not available\n",
      "arctangent_function\n",
      "logistic_function\n",
      "function not available\n",
      "hyperbolic_function\n",
      "function not available\n",
      "arctangent_function\n",
      "function not available\n",
      "gen_logistic_function\n",
      "function not available\n",
      "algebraic_function\n",
      "logistic_function\n",
      "function not available\n",
      "hyperbolic_function\n",
      "function not available\n",
      "arctangent_function\n",
      "function not available\n",
      "gen_logistic_function\n",
      "logistic_function\n",
      "function not available\n",
      "hyperbolic_function\n",
      "function not available\n",
      "arctangent_function\n",
      "function not available\n",
      "gen_logistic_function\n",
      "function not available\n",
      "algebraic_function\n",
      "function not available\n",
      "gudermannian_function\n",
      "================ TEST CASES =====================\n",
      "hyperbolic\n",
      "0.7615941559557649\n",
      "\n",
      " logistic\n",
      "0.7310585786300049\n",
      "\n",
      " arctangent\n",
      "0.7853981633974483\n",
      "\n",
      " algebraic\n",
      "0.7955508245341965\n",
      "\n",
      " gen_logistic\n",
      "0.2689414213699951\n",
      "\n",
      " gudermannian\n",
      "0.8657694832396586\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from project_part_2 import sigmoid_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "The Datasets selected for evaluation purposes are: \n",
    "- mandelon\n",
    "- musk \n",
    "\n",
    "Both datasets are accesbile from [add the name of the website]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Madelon Dataset\n",
    "\n",
    "- Data type: non-sparse\n",
    "- Number of features: 500\n",
    "- Number of examples and check-sums:\n",
    "      \t     Pos_ex\tNeg_ex\tTot_ex\tCheck_sum\n",
    "             \n",
    "\n",
    "     Train\t 1000\t 1000\t 2000\t488083511.00\n",
    "     \n",
    "\n",
    "     Valid\t  300\t  300\t  600\t146395833.00\n",
    "     \n",
    "\n",
    "     Test\t  900\t  900\t 1800\t439209553.00\n",
    "     \n",
    "\n",
    "     All  \t 2200\t 2200\t 4400\t1073688897.00\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madelon = pd.read_csv('madelon/madelon_train.data', sep=' ', header = None)\n",
    "madelon.info()\n",
    "madelon.head()\n",
    "\n",
    "madelon_test = pd.read_csv('madelon/madelon_test.data', sep=' ', header = None)\n",
    "madelon_test.info()\n",
    "#madelon_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musk Dataset\n",
    "\n",
    "4. Relevant Information:\n",
    "   This dataset describes a set of **92** molecules of which **47** are judged\n",
    "   by human experts to be musks and the remaining 45 molecules are\n",
    "   judged to be non-musks.  The goal is to learn to predict whether\n",
    "   new molecules will be musks or non-musks.  However, the 166 features\n",
    "   that describe these molecules depend upon the exact shape, or\n",
    "   conformation, of the molecule.  Because bonds can rotate, a single\n",
    "   molecule can adopt many different shapes.  To generate this data\n",
    "   set, the low-energy conformations of the molecules were generated\n",
    "   and then filtered to remove highly similar conformations. This left\n",
    "   476 conformations.  Then, a feature vector was extracted that\n",
    "   describes each conformation.\n",
    "\n",
    "   This many-to-one relationship between feature vectors and molecules\n",
    "   is called the **\"multiple instance problem\"**.  When learning a\n",
    "   classifier for this data, the classifier should classify a molecule\n",
    "   as \"musk\" if ANY of its conformations is classified as a musk.  A\n",
    "   molecule should be classified as \"non-musk\" if NONE of its\n",
    "   conformations is classified as a musk.\n",
    "\n",
    "5. Number of Instances  **476**\n",
    "\n",
    "6. Number of Attributes **168** plus the class.\n",
    "\n",
    "7. For Each Attribute:\n",
    "   \n",
    "   Attribute:           Description:\n",
    "   molecule_name:       Symbolic name of each molecule.  Musks have names such\n",
    "                        as MUSK-188.  Non-musks have names such as\n",
    "                        NON-MUSK-jp13.\n",
    "   conformation_name:   Symbolic name of each conformation.  These\n",
    "                        have the format MOL_ISO+CONF, where MOL is the\n",
    "                        molecule number, ISO is the stereoisomer\n",
    "                        number (usually 1), and CONF is the\n",
    "                        conformation number. \n",
    "   f1 through f162:     These are \"distance features\" along rays (see\n",
    "                        paper cited above).  The distances are\n",
    "                        measured in hundredths of Angstroms.  The\n",
    "                        distances may be negative or positive, since\n",
    "                        they are actually measured relative to an\n",
    "                        origin placed along each ray.  The origin was\n",
    "                        defined by a \"consensus musk\" surface that is\n",
    "                        no longer used.  Hence, any experiments with\n",
    "                        the data should treat these feature values as\n",
    "                        lying on an arbitrary continuous scale.  In\n",
    "                        particular, the algorithm should not make any\n",
    "                        use of the zero point or the sign of each\n",
    "                        feature value. \n",
    "   f163:                This is the distance of the oxygen atom in the\n",
    "                        molecule to a designated point in 3-space.\n",
    "                        This is also called OXY-DIS.\n",
    "   f164:                OXY-X: X-displacement from the designated\n",
    "                        point.\n",
    "   f165:                OXY-Y: Y-displacement from the designated\n",
    "                        point.\n",
    "   f166:                OXY-Z: Z-displacement from the designated\n",
    "                        point. \n",
    "   class:               0 => non-musk, 1 => musk\n",
    "\n",
    "   Please note that the molecule_name and conformation_name attributes\n",
    "   should not be used to predict the class.\n",
    "\n",
    "8. Missing Attribute Values: none.\n",
    "\n",
    "9. Class Distribution: \n",
    "   Musks:     47\n",
    "   Non-musks: 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "musk =  pd.read_csv('musk/clean2.data/clean2.data', header = None)\n",
    "musk.info()\n",
    "musk.head()\n",
    "\n",
    "musk_test =  pd.read_csv('musk/clean1.data/clean1.data', header = None)\n",
    "#musk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data for Classifiers. \n",
    "\n",
    "According to the shared paper [A jaya algorithm based wrapper method for optimal feature selection in supervised classification] data has to be trained using following classifiers: \n",
    "\n",
    "    - NB (Naive Bayes)\n",
    "    - KNN (K Nearest Neighbor)\n",
    "    - LDA \n",
    "    - RT (Regression Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a : Separating features and labels as X, Y variables  (Training Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Musk_X = musk.drop(168, axis = 1)\n",
    "Musk_X = Musk_X.drop(0, axis=1)\n",
    "Musk_X = Musk_X.drop(1, axis=1)\n",
    "Musk_X.columns = np.arange(len(Musk_X.columns))\n",
    "Musk_Y = musk[168]\n",
    "\n",
    "\n",
    "Madelon_X = madelon.drop(500, axis = 1)\n",
    "Y_labels = np.hstack([np.ones(1000), np.zeros(1000)])\n",
    "Madelon_Y = pd.Series(Y_labels.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b : Separating features and labels as X, Y variables  (Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Musk_x = musk_test.drop(168, axis = 1)\n",
    "Musk_x = Musk_x.drop(0, axis=1)\n",
    "Musk_x = Musk_x.drop(1, axis=1)\n",
    "Musk_x.columns = np.arange(len(Musk_x.columns))\n",
    "Musk_y = musk_test[168]\n",
    "\n",
    "\n",
    "Madelon_x = madelon_test.drop(500, axis = 1)\n",
    "y_labels = np.hstack([np.ones(900), np.zeros(900)])\n",
    "Madelon_y = pd.Series(y_labels.T)\n",
    "len(Musk_x)\n",
    "len(Musk_y)\n",
    "\n",
    "print(\"Madelon TRAINING \\n dataset: # of features: \" , Madelon_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_X.shape[0], \n",
    "      'Y_shape: ', Madelon_Y.shape)\n",
    "\n",
    "print(\"Madelon TEST \\n dataset: # of features: \" , Madelon_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_x.shape[0], \n",
    "      'Y_shape: ', Madelon_y.shape)\n",
    "\n",
    "print(\"Musk TRAINING \\n dataset: # of features: \" , Musk_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_X.shape[0],\n",
    "      'Y_shape: ', Musk_Y.shape)\n",
    "\n",
    "print(\"Musk TEST \\n dataset: # of features: \" , Musk_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_x.shape[0], \n",
    "      'Y_shape: ', Musk_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Classification\n",
    "\n",
    "#### Step 3a: Train Classifiers without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def data_classification(train_x, train_y, test_x = None ,test_y = None):\n",
    "    names = [\"NB\", \"RT\", \"KNN\", \"LDA\"]\n",
    "\n",
    "    classifiers = [\n",
    "        GaussianNB(),\n",
    "        DecisionTreeRegressor(),\n",
    "        KNeighborsClassifier(n_neighbors=10),\n",
    "        LinearDiscriminantAnalysis()\n",
    "    ]\n",
    "    model_return = {}\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        clf.fit(train_x, np.ravel(train_y))\n",
    "        print(clf)\n",
    "        \n",
    "        # TRAIN\n",
    "        #probs = clf.predict_proba(train_x)\n",
    "        pred_ytrain = clf.predict(train_x)\n",
    "        score_ytrain = clf.score(train_x,np.ravel(train_y))\n",
    "        \n",
    "        #TEST\n",
    "        #probs_t = clf.predict_proba(test_x)\n",
    "        pred_ytest = clf.predict(test_x)\n",
    "        score_ytest = clf.score(test_x,np.ravel(test_y))\n",
    "        \n",
    "        model = {'Name' : name,\n",
    "                'Classes': ['cancer', 'no cancer'],\n",
    "                'Date': '2020-10-01',\n",
    "                'Datasets': ['madelon', 'musk'],\n",
    "                'Training_Score' : score_ytrain,\n",
    "                 'Test_Score' : score_ytest\n",
    "                }\n",
    "        \n",
    "        #print(\"classifier %s, train score %.5f \\n\" %(name,score_ytrain))\n",
    "        #print(\"classifier %s, test score %.5f \\n\" %(name,score_ytest))\n",
    "        print(\"classifier %s \" %(name), classification_report(np.ravel(test_y), pred_ytest, digits = 4))\n",
    "        #print(\"probabilty of test\", probs)\n",
    "        model_return[name] = pred_ytrain\n",
    "    return model_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "madelon_return = data_classification(train_x = Madelon_X, train_y = Madelon_Y, test_x = Madelon_x ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "musk_return = data_classification(train_x = Musk_X, train_y = Musk_Y, test_x = Musk_x ,test_y = Musk_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Algorithm for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class feature_optimization:\n",
    "    def __init__(self, trainX, trainY, testX, testY, population, obj_function):\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.testX  = testX\n",
    "        self.testY  = testY\n",
    "        self.P = population\n",
    "        if obj_function == 'ER':\n",
    "            self.objective_function = fx_ER\n",
    "            self.jaya_function = self.jaya_ER\n",
    "        elif obj_function == 'AUC':\n",
    "            self.objective_function = fx_AUC\n",
    "            self.jaya_function = self.jaya_AUC\n",
    "        else:\n",
    "            print('please select the correct fx function-- ER [Error rate] or AUC [Area under the curve]')\n",
    "        \n",
    "        self.dsp_binary_data = generate_binary_dataset(self.P, self.objective_function)\n",
    "        \n",
    "        \n",
    "    def __call__(self):\n",
    "        self.X = self.dsp_binary_data(self.trainX, self.testX, self.trainY, self.testY)\n",
    "        optimized_population = self.jaya_function(self.X)\n",
    "        #print('OP_Features ', optimized_population.shape)\n",
    "        trainOP, testOP = self.optimized_training_test_sets(optimized_population)\n",
    "        return trainOP, testOP\n",
    "        \n",
    "    def __objective_function(self, X_new): \n",
    "        FV = []\n",
    "        for n in range(len(X_new)):\n",
    "            #print('xnew ', X_new.loc[n])\n",
    "            idx ,  = np.where(X_new.loc[n]==1)\n",
    "            \n",
    "            temp_train = self.trainX[idx]\n",
    "            temp_test = self.testX[idx]\n",
    "            \n",
    "           \n",
    "            FV.append(self.objective_function(X_train = temp_train,\n",
    "                          x_test  = temp_test,\n",
    "                          Y_train = self.trainY,\n",
    "                          y_test  = self.testY)\n",
    "                     )\n",
    "\n",
    "        return FV\n",
    "    \n",
    "    def optimized_training_test_sets(self, X):\n",
    "        idx,  = np.where(X.loc[0]==1)\n",
    "        #print(\"feature IDX \", idx)\n",
    "        temp_train = self.trainX[idx]\n",
    "        temp_test = self.testX[idx]\n",
    "        \n",
    "        return temp_train, temp_test\n",
    "   \n",
    "    \n",
    "\n",
    "    def binary_x_new(self,X_new):\n",
    "        '''\n",
    "        PURPOSE: to convert x_new value into binary [0, 1] \n",
    "        INPUT : output of Jaya algorithm line 36\n",
    "        OUTPUT: binary value of x_new\n",
    "        '''\n",
    "        #probability_jaya_x  = 1/(1 + np.exp(-2 * X_new) - 0.25)   # OUR PROPOSED ONE \n",
    "        probability_jaya_x  = 1/(1 + np.exp(-10 * X_new) - 0.5)    # PAPER FUNCTION\n",
    "        #print('probability_jaya_x ', probability_jaya_x)\n",
    "        random_r =  np.round(np.random.random(1),2)\n",
    "        prob = []\n",
    "        for p in probability_jaya_x:\n",
    "            if p > random_r:\n",
    "                prob.append(1.0)\n",
    "            else:\n",
    "                prob.append(0.0)\n",
    "        return prob    \n",
    "\n",
    "    def jaya_ER(self,x):\n",
    "        '''\n",
    "        INPUT:\n",
    "        bextX is with the smaller fx value, \n",
    "        worstX, \n",
    "        r1 and r2 are two random numbers between 0 and 1 \n",
    "        '''\n",
    "\n",
    "        MaxIter = 100\n",
    "        print('Processing .....')\n",
    "        for s in range(MaxIter):      # line 6\n",
    "            if len(x) >1:\n",
    "                \n",
    "                #print('=================    ITERATION ==============', s)\n",
    "                X_new = pd.DataFrame()\n",
    "                bestX = x['fx'].idxmin(axis=1)\n",
    "                worstX = x['fx'].idxmax(axis=1)\n",
    "                #print('BEST ', bestX, 'WORST ', worstX)\n",
    "                #print('INITIAL f(x) BEST ', x['fx'].min())\n",
    "                #print('INITIAL f(x) WORST ', x['fx'].max())\n",
    "                for i in range(len(x.columns)-1):\n",
    "                    r1 = np.round(np.random.random(1),2)\n",
    "                    r2 = np.round(np.random.random(1),2)\n",
    "\n",
    "                    jaya_X = (x[x.columns[i]] + r1 * (bestX - abs(x[x.columns[i]])) - r2*(worstX - abs(x[x.columns[i]])))\n",
    "\n",
    "                    X_new[x.columns[i]] = self.binary_x_new(jaya_X)\n",
    "\n",
    "                X_new['fx'] = self.__objective_function(X_new)\n",
    "                #print(X_new)\n",
    "\n",
    "                for d in range(len(x)):\n",
    "                    remove_index = []\n",
    "                    if(X_new['fx'].iloc[d] < x['fx'].iloc[d]):\n",
    "                        x['fx'].iloc[d] = X_new['fx'].iloc[d]\n",
    "                    \n",
    "                    else:\n",
    "                                              \n",
    "                        remove_index.append(d)\n",
    "                        \n",
    "                \n",
    "                x =x.drop(index=x.index[[remove_index]])\n",
    "            else:\n",
    "                print('Only one population left')\n",
    "                break;\n",
    "                #print(x)\n",
    "        return x\n",
    "\n",
    "    def jaya_AUC(self,x):\n",
    "        '''\n",
    "        INPUT:\n",
    "        bextX is with the larger fx value, \n",
    "        worstX, \n",
    "        r1 and r2 are two random numbers between 0 and 1 \n",
    "        '''\n",
    "\n",
    "        MaxIter = 100\n",
    "        print('Processing ........')\n",
    "        for s in range(MaxIter):      # line 6\n",
    "            # print(\"# of Populations \", len(x))\n",
    "            if len(x) >1:\n",
    "               \n",
    "                #print('=================    ITERATION ==============', s)\n",
    "                X_new = pd.DataFrame()\n",
    "                bestX = x['fx'].idxmax(axis=1)\n",
    "                worstX = x['fx'].idxmin(axis=1)\n",
    "                #print('BEST ', bestX, 'WORST ', worstX)\n",
    "                #print('INITIAL f(x) BEST ', x['fx'].max())\n",
    "                #print('INITIAL f(x) WORST ', x['fx'].min())\n",
    "                for i in range(len(x.columns)-1):\n",
    "                    r1 = np.round(np.random.random(1),2)\n",
    "                    r2 = np.round(np.random.random(1),2)\n",
    "\n",
    "                    jaya_X = (x[x.columns[i]] + r1 * (bestX - abs(x[x.columns[i]])) - r2*(worstX - abs(x[x.columns[i]])))\n",
    "\n",
    "                    X_new[x.columns[i]] = self.binary_x_new(jaya_X)\n",
    "\n",
    "                X_new['fx'] = self.__objective_function(X_new)\n",
    "                #print(X_new)\n",
    "\n",
    "                for d in range(len(x)):\n",
    "                    remove_index = []\n",
    "                    if(X_new['fx'].iloc[d] > x['fx'].iloc[d]):\n",
    "\n",
    "                        x['fx'].iloc[d] = X_new['fx'].iloc[d]\n",
    "                    else:\n",
    "                                              \n",
    "                        remove_index.append(d)\n",
    "                        \n",
    "                \n",
    "                x =x.drop(index=x.index[[remove_index]])\n",
    "            else:\n",
    "                print('Only one population left')\n",
    "                break;\n",
    "                \n",
    "        return x \n",
    "    \n",
    "def error_rate(pred, orig):\n",
    "    '''\n",
    "    PURPOSE: Calculate the rate of accuracy of a classifier. \n",
    "    INPUT: pred = predicted class by a classifier orig: original class. [type binary array]. \n",
    "    OUTPUT: float error rate. \n",
    "    \n",
    "    '''\n",
    "    error = np.mean((pred != orig).astype(float))\n",
    "    return error\n",
    "\n",
    "\n",
    "## NOTE: Covert fx_ER and fx_AUC into classes, so we can transfer the classifier as well. \n",
    "'''\n",
    "class fx_function_selection:\n",
    "    def __init__(self, model_selected, function_selected):\n",
    "        \n",
    "        \n",
    "        self.function_selected = function\n",
    "'''   \n",
    "def fx_ER(X_train, x_test, Y_train, y_test):\n",
    "    # We can choose any classifier here. \n",
    "    clf = GaussianNB()\n",
    "    #clf = KNeighborsClassifier(n_neighbors=10)\n",
    "    clf.fit(X_train, np.ravel(Y_train))\n",
    "    pred_y = clf.predict(x_test)\n",
    "    return error_rate(pred_y, y_test)\n",
    "\n",
    "def fx_AUC(X_train, x_test, Y_train, y_test):\n",
    "    # We can choose any classifier here.  \n",
    "    clf = GaussianNB()\n",
    "    #clf = KNeighborsClassifier(n_neighbors=10)\n",
    "    clf.fit(X_train, np.ravel(Y_train))\n",
    "    pred_y = clf.predict_proba(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    pred_y  = pred_y [:, 1]\n",
    "    lr_auc = roc_auc_score(y_test, pred_y )\n",
    "\n",
    "    return lr_auc\n",
    "\n",
    "class binary_sets:\n",
    "    def __init__(self, D):\n",
    "        self.D = D\n",
    "    def __call__(self, population):\n",
    "        idx = (population.keys())\n",
    "        binary = np.zeros(self.D)\n",
    "        np.put(binary, idx,1)\n",
    "        return binary\n",
    "    \n",
    "        \n",
    "class feature_subset:\n",
    "    def __init__(self, list_of_features):\n",
    "        self.features = list_of_features\n",
    "        \n",
    "        \n",
    "    def __call__(self, dataframe):\n",
    "        #columns = [f  for f in self.args]\n",
    "                \n",
    "        #print(self.features)  \n",
    "        new_df = dataframe[self.features]\n",
    "        return new_df \n",
    "    \n",
    "class generate_binary_dataset:\n",
    "    def __init__(self, population, fx):\n",
    "        self.P = population\n",
    "        self.f_x = fx\n",
    "        \n",
    "        return None\n",
    "    def __call__(self,X_train, x_test, Y_train, y_test):\n",
    "        D = len(X_train.columns)\n",
    "        col_names = X_train.columns\n",
    "        binary_dsp = binary_sets(D)\n",
    "        min_features = int(D/3)\n",
    "        \n",
    "        X = {}\n",
    "        FV = []\n",
    "        \n",
    "        \n",
    "        for i in range(self.P):\n",
    "            #print('=================================  Population: ', i, \"===============================\")\n",
    "            random_D = np.arange(2,(np.random.randint(min_features, D)))\n",
    "            #print(i, '# of features: ', len(random_D))\n",
    "            feature_sub = feature_subset(random_D)\n",
    "            \n",
    "            tem_train_X = feature_sub(X_train)\n",
    "            tem_test_X  = feature_sub(x_test)\n",
    "            \n",
    "            \n",
    "                       \n",
    "            FV.append(self.f_x(X_train = tem_train_X,\n",
    "                          x_test  = tem_test_X,\n",
    "                          Y_train = Y_train,\n",
    "                          y_test  = y_test)\n",
    "                     )\n",
    "            \n",
    "            \n",
    "            X[i] =  binary_dsp(tem_test_X)\n",
    "        Xx = pd.DataFrame(X.values(), columns = col_names ,index = X.keys())\n",
    "        \n",
    "        Xx['fx'] = FV\n",
    "        \n",
    "        #print(Xx.head(10))\n",
    "        return Xx\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING PROPOSED STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for a in range(10):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = feature_optimization(trainX = Musk_X,\n",
    "                               trainY = Musk_Y,\n",
    "                               testX = Musk_x,\n",
    "                               testY = Musk_y, \n",
    "                               population = 10,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MuskX_OP, Muskx_OP = dsp()\n",
    "    print('Number of Features ', MuskX_OP.shape[1])\n",
    "    musk_return = data_classification(train_x = MuskX_OP, train_y = Musk_Y, \n",
    "                                  test_x = Muskx_OP ,test_y = Musk_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in range(2):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = feature_optimization(trainX = Madelon_X,\n",
    "                               trainY = Madelon_Y,\n",
    "                               testX = Madelon_x,\n",
    "                               testY = Madelon_y, \n",
    "                               population = 30,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MadelonX_OP, Madelonx_OP = dsp()\n",
    "    print('Number of Features ', MadelonX_OP.shape[1])\n",
    "    Madelon_return = data_classification(train_x = MadelonX_OP, train_y = Madelon_Y, \n",
    "                                         test_x = Madelonx_OP ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
