{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose \n",
    "### Feature Selection by optimizing FSJaya Algorithm. \n",
    "\n",
    "The challenge is to tweak the present algorithm for feature selection purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from project_part_2 import sigmoid_function\n",
    "import project_part_1 as pj1\n",
    "import classifiers as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "The Datasets selected for evaluation purposes are: \n",
    "- mandelon\n",
    "- musk \n",
    "\n",
    "Both datasets are accesbile from [add the name of the website]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Madelon Dataset\n",
    "\n",
    "- Data type: non-sparse\n",
    "- Number of features: 500\n",
    "- Number of examples and check-sums:\n",
    "      \t     Pos_ex\tNeg_ex\tTot_ex\tCheck_sum\n",
    "             \n",
    "\n",
    "     Train\t 1000\t 1000\t 2000\t488083511.00\n",
    "     \n",
    "\n",
    "     Valid\t  300\t  300\t  600\t146395833.00\n",
    "     \n",
    "\n",
    "     Test\t  900\t  900\t 1800\t439209553.00\n",
    "     \n",
    "\n",
    "     All  \t 2200\t 2200\t 4400\t1073688897.00\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 501 entries, 0 to 500\n",
      "dtypes: float64(1), int64(500)\n",
      "memory usage: 7.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Columns: 501 entries, 0 to 500\n",
      "dtypes: float64(1), int64(500)\n",
      "memory usage: 6.9 MB\n"
     ]
    }
   ],
   "source": [
    "madelon = pd.read_csv('madelon/madelon_train.data', sep=' ', header = None)\n",
    "madelon.info()\n",
    "madelon.head()\n",
    "\n",
    "madelon_test = pd.read_csv('madelon/madelon_test.data', sep=' ', header = None)\n",
    "madelon_test.info()\n",
    "#madelon_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musk Dataset\n",
    "\n",
    "4. Relevant Information:\n",
    "   This dataset describes a set of **92** molecules of which **47** are judged\n",
    "   by human experts to be musks and the remaining 45 molecules are\n",
    "   judged to be non-musks.  The goal is to learn to predict whether\n",
    "   new molecules will be musks or non-musks.  However, the 166 features\n",
    "   that describe these molecules depend upon the exact shape, or\n",
    "   conformation, of the molecule.  Because bonds can rotate, a single\n",
    "   molecule can adopt many different shapes.  To generate this data\n",
    "   set, the low-energy conformations of the molecules were generated\n",
    "   and then filtered to remove highly similar conformations. This left\n",
    "   476 conformations.  Then, a feature vector was extracted that\n",
    "   describes each conformation.\n",
    "\n",
    "   This many-to-one relationship between feature vectors and molecules\n",
    "   is called the **\"multiple instance problem\"**.  When learning a\n",
    "   classifier for this data, the classifier should classify a molecule\n",
    "   as \"musk\" if ANY of its conformations is classified as a musk.  A\n",
    "   molecule should be classified as \"non-musk\" if NONE of its\n",
    "   conformations is classified as a musk.\n",
    "\n",
    "5. Number of Instances  **476**\n",
    "\n",
    "6. Number of Attributes **168** plus the class.\n",
    "\n",
    "7. For Each Attribute:\n",
    "   \n",
    "   Attribute:           Description:\n",
    "   molecule_name:       Symbolic name of each molecule.  Musks have names such\n",
    "                        as MUSK-188.  Non-musks have names such as\n",
    "                        NON-MUSK-jp13.\n",
    "   conformation_name:   Symbolic name of each conformation.  These\n",
    "                        have the format MOL_ISO+CONF, where MOL is the\n",
    "                        molecule number, ISO is the stereoisomer\n",
    "                        number (usually 1), and CONF is the\n",
    "                        conformation number. \n",
    "   f1 through f162:     These are \"distance features\" along rays (see\n",
    "                        paper cited above).  The distances are\n",
    "                        measured in hundredths of Angstroms.  The\n",
    "                        distances may be negative or positive, since\n",
    "                        they are actually measured relative to an\n",
    "                        origin placed along each ray.  The origin was\n",
    "                        defined by a \"consensus musk\" surface that is\n",
    "                        no longer used.  Hence, any experiments with\n",
    "                        the data should treat these feature values as\n",
    "                        lying on an arbitrary continuous scale.  In\n",
    "                        particular, the algorithm should not make any\n",
    "                        use of the zero point or the sign of each\n",
    "                        feature value. \n",
    "   f163:                This is the distance of the oxygen atom in the\n",
    "                        molecule to a designated point in 3-space.\n",
    "                        This is also called OXY-DIS.\n",
    "   f164:                OXY-X: X-displacement from the designated\n",
    "                        point.\n",
    "   f165:                OXY-Y: Y-displacement from the designated\n",
    "                        point.\n",
    "   f166:                OXY-Z: Z-displacement from the designated\n",
    "                        point. \n",
    "   class:               0 => non-musk, 1 => musk\n",
    "\n",
    "   Please note that the molecule_name and conformation_name attributes\n",
    "   should not be used to predict the class.\n",
    "\n",
    "8. Missing Attribute Values: none.\n",
    "\n",
    "9. Class Distribution: \n",
    "   Musks:     47\n",
    "   Non-musks: 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6598 entries, 0 to 6597\n",
      "Columns: 169 entries, 0 to 168\n",
      "dtypes: float64(1), int64(166), object(2)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "musk =  pd.read_csv('musk/clean2.data/clean2.data', header = None)\n",
    "musk.info()\n",
    "musk.head()\n",
    "\n",
    "musk_test =  pd.read_csv('musk/clean1.data/clean1.data', header = None)\n",
    "#musk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data for Classifiers. \n",
    "\n",
    "According to the shared paper [A jaya algorithm based wrapper method for optimal feature selection in supervised classification] data has to be trained using following classifiers: \n",
    "\n",
    "    - NB (Naive Bayes)\n",
    "    - KNN (K Nearest Neighbor)\n",
    "    - LDA \n",
    "    - RT (Regression Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a : Separating features and labels as X, Y variables  (Training Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Musk_X = musk.drop(168, axis = 1)\n",
    "Musk_X = Musk_X.drop(0, axis=1)\n",
    "Musk_X = Musk_X.drop(1, axis=1)\n",
    "Musk_X.columns = np.arange(len(Musk_X.columns))\n",
    "Musk_Y = musk[168]\n",
    "\n",
    "\n",
    "Madelon_X = madelon.drop(500, axis = 1)\n",
    "Y_labels = np.hstack([np.ones(1000), np.zeros(1000)])\n",
    "Madelon_Y = pd.Series(Y_labels.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b : Separating features and labels as X, Y variables  (Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madelon TRAINING \n",
      " dataset: # of features:  500 \n",
      " Number of Measurements:  2000 Y_shape:  (2000,)\n",
      "Madelon TEST \n",
      " dataset: # of features:  500 \n",
      " Number of Measurements:  1800 Y_shape:  (1800,)\n",
      "Musk TRAINING \n",
      " dataset: # of features:  166 \n",
      " Number of Measurements:  6598 Y_shape:  (6598,)\n",
      "Musk TEST \n",
      " dataset: # of features:  166 \n",
      " Number of Measurements:  476 Y_shape:  (476,)\n"
     ]
    }
   ],
   "source": [
    "Musk_x = musk_test.drop(168, axis = 1)\n",
    "Musk_x = Musk_x.drop(0, axis=1)\n",
    "Musk_x = Musk_x.drop(1, axis=1)\n",
    "Musk_x.columns = np.arange(len(Musk_x.columns))\n",
    "Musk_y = musk_test[168]\n",
    "\n",
    "\n",
    "Madelon_x = madelon_test.drop(500, axis = 1)\n",
    "y_labels = np.hstack([np.ones(900), np.zeros(900)])\n",
    "Madelon_y = pd.Series(y_labels.T)\n",
    "len(Musk_x)\n",
    "len(Musk_y)\n",
    "\n",
    "print(\"Madelon TRAINING \\n dataset: # of features: \" , Madelon_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_X.shape[0], \n",
    "      'Y_shape: ', Madelon_Y.shape)\n",
    "\n",
    "print(\"Madelon TEST \\n dataset: # of features: \" , Madelon_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_x.shape[0], \n",
    "      'Y_shape: ', Madelon_y.shape)\n",
    "\n",
    "print(\"Musk TRAINING \\n dataset: # of features: \" , Musk_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_X.shape[0],\n",
    "      'Y_shape: ', Musk_Y.shape)\n",
    "\n",
    "print(\"Musk TEST \\n dataset: # of features: \" , Musk_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_x.shape[0], \n",
    "      'Y_shape: ', Musk_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Classification\n",
    "\n",
    "#### Step 3a: Train Classifiers without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4959    0.4667    0.4808       900\n",
      "         1.0     0.4963    0.5256    0.5105       900\n",
      "\n",
      "    accuracy                         0.4961      1800\n",
      "   macro avg     0.4961    0.4961    0.4957      1800\n",
      "weighted avg     0.4961    0.4961    0.4957      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5057    0.4967    0.5011       900\n",
      "         1.0     0.5055    0.5144    0.5099       900\n",
      "\n",
      "    accuracy                         0.5056      1800\n",
      "   macro avg     0.5056    0.5056    0.5055      1800\n",
      "weighted avg     0.5056    0.5056    0.5055      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5031    0.6244    0.5573       900\n",
      "         1.0     0.5051    0.3833    0.4359       900\n",
      "\n",
      "    accuracy                         0.5039      1800\n",
      "   macro avg     0.5041    0.5039    0.4966      1800\n",
      "weighted avg     0.5041    0.5039    0.4966      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5137    0.5000    0.5068       900\n",
      "         1.0     0.5130    0.5267    0.5197       900\n",
      "\n",
      "    accuracy                         0.5133      1800\n",
      "   macro avg     0.5133    0.5133    0.5132      1800\n",
      "weighted avg     0.5133    0.5133    0.5132      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "madelon_return = cl.data_classification(train_x = Madelon_X, train_y = Madelon_Y, test_x = Madelon_x ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6724    0.8773    0.7613       269\n",
      "         1.0     0.7360    0.4444    0.5542       207\n",
      "\n",
      "    accuracy                         0.6891       476\n",
      "   macro avg     0.7042    0.6609    0.6578       476\n",
      "weighted avg     0.7000    0.6891    0.6712       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7580    0.4424    0.5587       269\n",
      "         1.0     0.5298    0.8164    0.6426       207\n",
      "\n",
      "    accuracy                         0.6050       476\n",
      "   macro avg     0.6439    0.6294    0.6006       476\n",
      "weighted avg     0.6587    0.6050    0.5952       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9004    0.9405    0.9200       269\n",
      "         1.0     0.9179    0.8647    0.8905       207\n",
      "\n",
      "    accuracy                         0.9076       476\n",
      "   macro avg     0.9092    0.9026    0.9053       476\n",
      "weighted avg     0.9080    0.9076    0.9072       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8111    0.8141    0.8126       269\n",
      "         1.0     0.7573    0.7536    0.7554       207\n",
      "\n",
      "    accuracy                         0.7878       476\n",
      "   macro avg     0.7842    0.7839    0.7840       476\n",
      "weighted avg     0.7877    0.7878    0.7878       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "musk_return = cl.data_classification(train_x = Musk_X, train_y = Musk_Y, test_x = Musk_x ,test_y = Musk_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING PROPOSED STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      " -----------RESULT # -------  0\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  121\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7079    0.8290    0.7637       269\n",
      "         1.0     0.7143    0.5556    0.6250       207\n",
      "\n",
      "    accuracy                         0.7101       476\n",
      "   macro avg     0.7111    0.6923    0.6943       476\n",
      "weighted avg     0.7107    0.7101    0.7034       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8113    0.7993    0.8052       269\n",
      "         1.0     0.7441    0.7585    0.7512       207\n",
      "\n",
      "    accuracy                         0.7815       476\n",
      "   macro avg     0.7777    0.7789    0.7782       476\n",
      "weighted avg     0.7821    0.7815    0.7817       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8660    0.9368    0.9000       269\n",
      "         1.0     0.9081    0.8116    0.8571       207\n",
      "\n",
      "    accuracy                         0.8824       476\n",
      "   macro avg     0.8870    0.8742    0.8786       476\n",
      "weighted avg     0.8843    0.8824    0.8814       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7633    0.8030    0.7826       269\n",
      "         1.0     0.7254    0.6763    0.7000       207\n",
      "\n",
      "    accuracy                         0.7479       476\n",
      "   macro avg     0.7443    0.7397    0.7413       476\n",
      "weighted avg     0.7468    0.7479    0.7467       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  1\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  152\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7147    0.8476    0.7755       269\n",
      "         1.0     0.7389    0.5604    0.6374       207\n",
      "\n",
      "    accuracy                         0.7227       476\n",
      "   macro avg     0.7268    0.7040    0.7064       476\n",
      "weighted avg     0.7252    0.7227    0.7154       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7647    0.7732    0.7689       269\n",
      "         1.0     0.7010    0.6908    0.6959       207\n",
      "\n",
      "    accuracy                         0.7374       476\n",
      "   macro avg     0.7328    0.7320    0.7324       476\n",
      "weighted avg     0.7370    0.7374    0.7372       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8601    0.9368    0.8968       269\n",
      "         1.0     0.9071    0.8019    0.8513       207\n",
      "\n",
      "    accuracy                         0.8782       476\n",
      "   macro avg     0.8836    0.8694    0.8740       476\n",
      "weighted avg     0.8805    0.8782    0.8770       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7607    0.8625    0.8084       269\n",
      "         1.0     0.7836    0.6473    0.7090       207\n",
      "\n",
      "    accuracy                         0.7689       476\n",
      "   macro avg     0.7721    0.7549    0.7587       476\n",
      "weighted avg     0.7706    0.7689    0.7651       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  2\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  143\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7048    0.8253    0.7603       269\n",
      "         1.0     0.7081    0.5507    0.6196       207\n",
      "\n",
      "    accuracy                         0.7059       476\n",
      "   macro avg     0.7064    0.6880    0.6899       476\n",
      "weighted avg     0.7062    0.7059    0.6991       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7732    0.7732    0.7732       269\n",
      "         1.0     0.7053    0.7053    0.7053       207\n",
      "\n",
      "    accuracy                         0.7437       476\n",
      "   macro avg     0.7393    0.7393    0.7393       476\n",
      "weighted avg     0.7437    0.7437    0.7437       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8711    0.9294    0.8993       269\n",
      "         1.0     0.8995    0.8213    0.8586       207\n",
      "\n",
      "    accuracy                         0.8824       476\n",
      "   macro avg     0.8853    0.8753    0.8789       476\n",
      "weighted avg     0.8834    0.8824    0.8816       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7685    0.8513    0.8078       269\n",
      "         1.0     0.7753    0.6667    0.7169       207\n",
      "\n",
      "    accuracy                         0.7710       476\n",
      "   macro avg     0.7719    0.7590    0.7623       476\n",
      "weighted avg     0.7714    0.7710    0.7682       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  3\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  144\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7022    0.8327    0.7619       269\n",
      "         1.0     0.7134    0.5411    0.6154       207\n",
      "\n",
      "    accuracy                         0.7059       476\n",
      "   macro avg     0.7078    0.6869    0.6886       476\n",
      "weighted avg     0.7071    0.7059    0.6982       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7887    0.7770    0.7828       269\n",
      "         1.0     0.7156    0.7295    0.7225       207\n",
      "\n",
      "    accuracy                         0.7563       476\n",
      "   macro avg     0.7522    0.7532    0.7526       476\n",
      "weighted avg     0.7569    0.7563    0.7566       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8711    0.9294    0.8993       269\n",
      "         1.0     0.8995    0.8213    0.8586       207\n",
      "\n",
      "    accuracy                         0.8824       476\n",
      "   macro avg     0.8853    0.8753    0.8789       476\n",
      "weighted avg     0.8834    0.8824    0.8816       476\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7641    0.8550    0.8070       269\n",
      "         1.0     0.7771    0.6570    0.7120       207\n",
      "\n",
      "    accuracy                         0.7689       476\n",
      "   macro avg     0.7706    0.7560    0.7595       476\n",
      "weighted avg     0.7698    0.7689    0.7657       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  4\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  74\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7165    0.8550    0.7797       269\n",
      "         1.0     0.7484    0.5604    0.6409       207\n",
      "\n",
      "    accuracy                         0.7269       476\n",
      "   macro avg     0.7324    0.7077    0.7103       476\n",
      "weighted avg     0.7304    0.7269    0.7193       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8521    0.8141    0.8327       269\n",
      "         1.0     0.7717    0.8164    0.7934       207\n",
      "\n",
      "    accuracy                         0.8151       476\n",
      "   macro avg     0.8119    0.8153    0.8131       476\n",
      "weighted avg     0.8172    0.8151    0.8156       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8485    0.9368    0.8905       269\n",
      "         1.0     0.9050    0.7826    0.8394       207\n",
      "\n",
      "    accuracy                         0.8697       476\n",
      "   macro avg     0.8768    0.8597    0.8649       476\n",
      "weighted avg     0.8731    0.8697    0.8682       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7148    0.7918    0.7513       269\n",
      "         1.0     0.6854    0.5894    0.6338       207\n",
      "\n",
      "    accuracy                         0.7038       476\n",
      "   macro avg     0.7001    0.6906    0.6925       476\n",
      "weighted avg     0.7020    0.7038    0.7002       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  5\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  61\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6734    0.8662    0.7577       269\n",
      "         1.0     0.7231    0.4541    0.5579       207\n",
      "\n",
      "    accuracy                         0.6870       476\n",
      "   macro avg     0.6982    0.6601    0.6578       476\n",
      "weighted avg     0.6950    0.6870    0.6708       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8051    0.8290    0.8168       269\n",
      "         1.0     0.7688    0.7391    0.7537       207\n",
      "\n",
      "    accuracy                         0.7899       476\n",
      "   macro avg     0.7869    0.7841    0.7853       476\n",
      "weighted avg     0.7893    0.7899    0.7894       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8306    0.9480    0.8854       269\n",
      "         1.0     0.9172    0.7488    0.8245       207\n",
      "\n",
      "    accuracy                         0.8613       476\n",
      "   macro avg     0.8739    0.8484    0.8549       476\n",
      "weighted avg     0.8683    0.8613    0.8589       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6923    0.8364    0.7576       269\n",
      "         1.0     0.7086    0.5169    0.5978       207\n",
      "\n",
      "    accuracy                         0.6975       476\n",
      "   macro avg     0.7005    0.6767    0.6777       476\n",
      "weighted avg     0.6994    0.6975    0.6881       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  6\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  62\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6714    0.8736    0.7593       269\n",
      "         1.0     0.7302    0.4444    0.5526       207\n",
      "\n",
      "    accuracy                         0.6870       476\n",
      "   macro avg     0.7008    0.6590    0.6559       476\n",
      "weighted avg     0.6970    0.6870    0.6694       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8468    0.7807    0.8124       269\n",
      "         1.0     0.7412    0.8164    0.7770       207\n",
      "\n",
      "    accuracy                         0.7962       476\n",
      "   macro avg     0.7940    0.7985    0.7947       476\n",
      "weighted avg     0.8009    0.7962    0.7970       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8361    0.9480    0.8885       269\n",
      "         1.0     0.9181    0.7585    0.8307       207\n",
      "\n",
      "    accuracy                         0.8655       476\n",
      "   macro avg     0.8771    0.8532    0.8596       476\n",
      "weighted avg     0.8718    0.8655    0.8634       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6942    0.8439    0.7617       269\n",
      "         1.0     0.7181    0.5169    0.6011       207\n",
      "\n",
      "    accuracy                         0.7017       476\n",
      "   macro avg     0.7062    0.6804    0.6814       476\n",
      "weighted avg     0.7046    0.7017    0.6919       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  7\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  130\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7037    0.8476    0.7690       269\n",
      "         1.0     0.7303    0.5362    0.6184       207\n",
      "\n",
      "    accuracy                         0.7122       476\n",
      "   macro avg     0.7170    0.6919    0.6937       476\n",
      "weighted avg     0.7153    0.7122    0.7035       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7912    0.8030    0.7970       269\n",
      "         1.0     0.7389    0.7246    0.7317       207\n",
      "\n",
      "    accuracy                         0.7689       476\n",
      "   macro avg     0.7651    0.7638    0.7644       476\n",
      "weighted avg     0.7685    0.7689    0.7686       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8664    0.9405    0.9020       269\n",
      "         1.0     0.9130    0.8116    0.8593       207\n",
      "\n",
      "    accuracy                         0.8845       476\n",
      "   macro avg     0.8897    0.8761    0.8806       476\n",
      "weighted avg     0.8867    0.8845    0.8834       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7616    0.8550    0.8056       269\n",
      "         1.0     0.7759    0.6522    0.7087       207\n",
      "\n",
      "    accuracy                         0.7668       476\n",
      "   macro avg     0.7687    0.7536    0.7571       476\n",
      "weighted avg     0.7678    0.7668    0.7634       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  8\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  67\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7003    0.8253    0.7577       269\n",
      "         1.0     0.7044    0.5411    0.6120       207\n",
      "\n",
      "    accuracy                         0.7017       476\n",
      "   macro avg     0.7024    0.6832    0.6849       476\n",
      "weighted avg     0.7021    0.7017    0.6943       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8521    0.8141    0.8327       269\n",
      "         1.0     0.7717    0.8164    0.7934       207\n",
      "\n",
      "    accuracy                         0.8151       476\n",
      "   macro avg     0.8119    0.8153    0.8131       476\n",
      "weighted avg     0.8172    0.8151    0.8156       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8537    0.9331    0.8917       269\n",
      "         1.0     0.9011    0.7923    0.8432       207\n",
      "\n",
      "    accuracy                         0.8718       476\n",
      "   macro avg     0.8774    0.8627    0.8674       476\n",
      "weighted avg     0.8743    0.8718    0.8706       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7105    0.8030    0.7539       269\n",
      "         1.0     0.6919    0.5749    0.6280       207\n",
      "\n",
      "    accuracy                         0.7038       476\n",
      "   macro avg     0.7012    0.6889    0.6909       476\n",
      "weighted avg     0.7024    0.7038    0.6992       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  9\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  148\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7129    0.8401    0.7713       269\n",
      "         1.0     0.7296    0.5604    0.6339       207\n",
      "\n",
      "    accuracy                         0.7185       476\n",
      "   macro avg     0.7212    0.7003    0.7026       476\n",
      "weighted avg     0.7202    0.7185    0.7116       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7601    0.7658    0.7630       269\n",
      "         1.0     0.6927    0.6860    0.6893       207\n",
      "\n",
      "    accuracy                         0.7311       476\n",
      "   macro avg     0.7264    0.7259    0.7261       476\n",
      "weighted avg     0.7308    0.7311    0.7309       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8660    0.9368    0.9000       269\n",
      "         1.0     0.9081    0.8116    0.8571       207\n",
      "\n",
      "    accuracy                         0.8824       476\n",
      "   macro avg     0.8870    0.8742    0.8786       476\n",
      "weighted avg     0.8843    0.8824    0.8814       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7692    0.8550    0.8099       269\n",
      "         1.0     0.7797    0.6667    0.7188       207\n",
      "\n",
      "    accuracy                         0.7731       476\n",
      "   macro avg     0.7744    0.7608    0.7643       476\n",
      "weighted avg     0.7738    0.7731    0.7702       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in range(10):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = pj1.feature_optimization(trainX = Musk_X,\n",
    "                               trainY = Musk_Y,\n",
    "                               testX = Musk_x,\n",
    "                               testY = Musk_y, \n",
    "                               population = 10,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MuskX_OP, Muskx_OP = dsp()\n",
    "    print('Number of Features ', MuskX_OP.shape[1])\n",
    "    musk_return = cl.data_classification(train_x = MuskX_OP, train_y = Musk_Y, \n",
    "                                  test_x = Muskx_OP ,test_y = Musk_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      " -----------RESULT # -------  0\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  220\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5076    0.4811    0.4940       900\n",
      "         1.0     0.5069    0.5333    0.5198       900\n",
      "\n",
      "    accuracy                         0.5072      1800\n",
      "   macro avg     0.5072    0.5072    0.5069      1800\n",
      "weighted avg     0.5072    0.5072    0.5069      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5010    0.5389    0.5193       900\n",
      "         1.0     0.5012    0.4633    0.4815       900\n",
      "\n",
      "    accuracy                         0.5011      1800\n",
      "   macro avg     0.5011    0.5011    0.5004      1800\n",
      "weighted avg     0.5011    0.5011    0.5004      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4955    0.6178    0.5500       900\n",
      "         1.0     0.4926    0.3711    0.4233       900\n",
      "\n",
      "    accuracy                         0.4944      1800\n",
      "   macro avg     0.4941    0.4944    0.4866      1800\n",
      "weighted avg     0.4941    0.4944    0.4866      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5056    0.5011    0.5033       900\n",
      "         1.0     0.5055    0.5100    0.5077       900\n",
      "\n",
      "    accuracy                         0.5056      1800\n",
      "   macro avg     0.5056    0.5056    0.5055      1800\n",
      "weighted avg     0.5056    0.5056    0.5055      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  1\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  263\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4988    0.4689    0.4834       900\n",
      "         1.0     0.4990    0.5289    0.5135       900\n",
      "\n",
      "    accuracy                         0.4989      1800\n",
      "   macro avg     0.4989    0.4989    0.4984      1800\n",
      "weighted avg     0.4989    0.4989    0.4984      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5094    0.5100    0.5097       900\n",
      "         1.0     0.5095    0.5089    0.5092       900\n",
      "\n",
      "    accuracy                         0.5094      1800\n",
      "   macro avg     0.5094    0.5094    0.5094      1800\n",
      "weighted avg     0.5094    0.5094    0.5094      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4930    0.5878    0.5362       900\n",
      "         1.0     0.4897    0.3956    0.4376       900\n",
      "\n",
      "    accuracy                         0.4917      1800\n",
      "   macro avg     0.4913    0.4917    0.4869      1800\n",
      "weighted avg     0.4913    0.4917    0.4869      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5067    0.5022    0.5045       900\n",
      "         1.0     0.5066    0.5111    0.5088       900\n",
      "\n",
      "    accuracy                         0.5067      1800\n",
      "   macro avg     0.5067    0.5067    0.5067      1800\n",
      "weighted avg     0.5067    0.5067    0.5067      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in range(2):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = pj1.feature_optimization(trainX = Madelon_X,\n",
    "                               trainY = Madelon_Y,\n",
    "                               testX = Madelon_x,\n",
    "                               testY = Madelon_y, \n",
    "                               population = 30,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MadelonX_OP, Madelonx_OP = dsp()\n",
    "    print('Number of Features ', MadelonX_OP.shape[1])\n",
    "    Madelon_return = cl.data_classification(train_x = MadelonX_OP, train_y = Madelon_Y, \n",
    "                                         test_x = Madelonx_OP ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
