{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose \n",
    "### Feature Selection by optimizing FSJaya Algorithm. \n",
    "\n",
    "The challenge is to tweak the present algorithm for feature selection purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from project_part_2 import sigmoid_function\n",
    "import project_part_1 as pj1\n",
    "import classifiers as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "The Datasets selected for evaluation purposes are: \n",
    "- mandelon\n",
    "- musk \n",
    "\n",
    "Both datasets are accesbile from [add the name of the website]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Madelon Dataset\n",
    "\n",
    "- Data type: non-sparse\n",
    "- Number of features: 500\n",
    "- Number of examples and check-sums:\n",
    "      \t     Pos_ex\tNeg_ex\tTot_ex\tCheck_sum\n",
    "             \n",
    "\n",
    "     Train\t 1000\t 1000\t 2000\t488083511.00\n",
    "     \n",
    "\n",
    "     Valid\t  300\t  300\t  600\t146395833.00\n",
    "     \n",
    "\n",
    "     Test\t  900\t  900\t 1800\t439209553.00\n",
    "     \n",
    "\n",
    "     All  \t 2200\t 2200\t 4400\t1073688897.00\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 501 entries, 0 to 500\n",
      "dtypes: float64(1), int64(500)\n",
      "memory usage: 7.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Columns: 501 entries, 0 to 500\n",
      "dtypes: float64(1), int64(500)\n",
      "memory usage: 6.9 MB\n"
     ]
    }
   ],
   "source": [
    "madelon = pd.read_csv('madelon/madelon_train.data', sep=' ', header = None)\n",
    "madelon.info()\n",
    "madelon.head()\n",
    "\n",
    "madelon_test = pd.read_csv('madelon/madelon_test.data', sep=' ', header = None)\n",
    "madelon_test.info()\n",
    "#madelon_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musk Dataset\n",
    "\n",
    "4. Relevant Information:\n",
    "   This dataset describes a set of **92** molecules of which **47** are judged\n",
    "   by human experts to be musks and the remaining 45 molecules are\n",
    "   judged to be non-musks.  The goal is to learn to predict whether\n",
    "   new molecules will be musks or non-musks.  However, the 166 features\n",
    "   that describe these molecules depend upon the exact shape, or\n",
    "   conformation, of the molecule.  Because bonds can rotate, a single\n",
    "   molecule can adopt many different shapes.  To generate this data\n",
    "   set, the low-energy conformations of the molecules were generated\n",
    "   and then filtered to remove highly similar conformations. This left\n",
    "   476 conformations.  Then, a feature vector was extracted that\n",
    "   describes each conformation.\n",
    "\n",
    "   This many-to-one relationship between feature vectors and molecules\n",
    "   is called the **\"multiple instance problem\"**.  When learning a\n",
    "   classifier for this data, the classifier should classify a molecule\n",
    "   as \"musk\" if ANY of its conformations is classified as a musk.  A\n",
    "   molecule should be classified as \"non-musk\" if NONE of its\n",
    "   conformations is classified as a musk.\n",
    "\n",
    "5. Number of Instances  **476**\n",
    "\n",
    "6. Number of Attributes **168** plus the class.\n",
    "\n",
    "7. For Each Attribute:\n",
    "   \n",
    "   Attribute:           Description:\n",
    "   molecule_name:       Symbolic name of each molecule.  Musks have names such\n",
    "                        as MUSK-188.  Non-musks have names such as\n",
    "                        NON-MUSK-jp13.\n",
    "   conformation_name:   Symbolic name of each conformation.  These\n",
    "                        have the format MOL_ISO+CONF, where MOL is the\n",
    "                        molecule number, ISO is the stereoisomer\n",
    "                        number (usually 1), and CONF is the\n",
    "                        conformation number. \n",
    "   f1 through f162:     These are \"distance features\" along rays (see\n",
    "                        paper cited above).  The distances are\n",
    "                        measured in hundredths of Angstroms.  The\n",
    "                        distances may be negative or positive, since\n",
    "                        they are actually measured relative to an\n",
    "                        origin placed along each ray.  The origin was\n",
    "                        defined by a \"consensus musk\" surface that is\n",
    "                        no longer used.  Hence, any experiments with\n",
    "                        the data should treat these feature values as\n",
    "                        lying on an arbitrary continuous scale.  In\n",
    "                        particular, the algorithm should not make any\n",
    "                        use of the zero point or the sign of each\n",
    "                        feature value. \n",
    "   f163:                This is the distance of the oxygen atom in the\n",
    "                        molecule to a designated point in 3-space.\n",
    "                        This is also called OXY-DIS.\n",
    "   f164:                OXY-X: X-displacement from the designated\n",
    "                        point.\n",
    "   f165:                OXY-Y: Y-displacement from the designated\n",
    "                        point.\n",
    "   f166:                OXY-Z: Z-displacement from the designated\n",
    "                        point. \n",
    "   class:               0 => non-musk, 1 => musk\n",
    "\n",
    "   Please note that the molecule_name and conformation_name attributes\n",
    "   should not be used to predict the class.\n",
    "\n",
    "8. Missing Attribute Values: none.\n",
    "\n",
    "9. Class Distribution: \n",
    "   Musks:     47\n",
    "   Non-musks: 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6598 entries, 0 to 6597\n",
      "Columns: 169 entries, 0 to 168\n",
      "dtypes: float64(1), int64(166), object(2)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "musk =  pd.read_csv('musk/clean2.data/clean2.data', header = None)\n",
    "musk.info()\n",
    "musk.head()\n",
    "\n",
    "musk_test =  pd.read_csv('musk/clean1.data/clean1.data', header = None)\n",
    "#musk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data for Classifiers. \n",
    "\n",
    "According to the shared paper [A jaya algorithm based wrapper method for optimal feature selection in supervised classification] data has to be trained using following classifiers: \n",
    "\n",
    "    - NB (Naive Bayes)\n",
    "    - KNN (K Nearest Neighbor)\n",
    "    - LDA \n",
    "    - RT (Regression Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a : Separating features and labels as X, Y variables  (Training Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Musk_X = musk.drop(168, axis = 1)\n",
    "Musk_X = Musk_X.drop(0, axis=1)\n",
    "Musk_X = Musk_X.drop(1, axis=1)\n",
    "Musk_X.columns = np.arange(len(Musk_X.columns))\n",
    "Musk_Y = musk[168]\n",
    "\n",
    "\n",
    "Madelon_X = madelon.drop(500, axis = 1)\n",
    "Y_labels = np.hstack([np.ones(1000), np.zeros(1000)])\n",
    "Madelon_Y = pd.Series(Y_labels.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b : Separating features and labels as X, Y variables  (Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madelon TRAINING \n",
      " dataset: # of features:  500 \n",
      " Number of Measurements:  2000 Y_shape:  (2000,)\n",
      "Madelon TEST \n",
      " dataset: # of features:  500 \n",
      " Number of Measurements:  1800 Y_shape:  (1800,)\n",
      "Musk TRAINING \n",
      " dataset: # of features:  166 \n",
      " Number of Measurements:  6598 Y_shape:  (6598,)\n",
      "Musk TEST \n",
      " dataset: # of features:  166 \n",
      " Number of Measurements:  476 Y_shape:  (476,)\n"
     ]
    }
   ],
   "source": [
    "Musk_x = musk_test.drop(168, axis = 1)\n",
    "Musk_x = Musk_x.drop(0, axis=1)\n",
    "Musk_x = Musk_x.drop(1, axis=1)\n",
    "Musk_x.columns = np.arange(len(Musk_x.columns))\n",
    "Musk_y = musk_test[168]\n",
    "\n",
    "\n",
    "Madelon_x = madelon_test.drop(500, axis = 1)\n",
    "y_labels = np.hstack([np.ones(900), np.zeros(900)])\n",
    "Madelon_y = pd.Series(y_labels.T)\n",
    "len(Musk_x)\n",
    "len(Musk_y)\n",
    "\n",
    "print(\"Madelon TRAINING \\n dataset: # of features: \" , Madelon_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_X.shape[0], \n",
    "      'Y_shape: ', Madelon_Y.shape)\n",
    "\n",
    "print(\"Madelon TEST \\n dataset: # of features: \" , Madelon_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Madelon_x.shape[0], \n",
    "      'Y_shape: ', Madelon_y.shape)\n",
    "\n",
    "print(\"Musk TRAINING \\n dataset: # of features: \" , Musk_X.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_X.shape[0],\n",
    "      'Y_shape: ', Musk_Y.shape)\n",
    "\n",
    "print(\"Musk TEST \\n dataset: # of features: \" , Musk_x.shape[1], \n",
    "      '\\n Number of Measurements: ', Musk_x.shape[0], \n",
    "      'Y_shape: ', Musk_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Classification\n",
    "\n",
    "#### Step 3a: Train Classifiers without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4959    0.4667    0.4808       900\n",
      "         1.0     0.4963    0.5256    0.5105       900\n",
      "\n",
      "    accuracy                         0.4961      1800\n",
      "   macro avg     0.4961    0.4961    0.4957      1800\n",
      "weighted avg     0.4961    0.4961    0.4957      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5085    0.4978    0.5031       900\n",
      "         1.0     0.5082    0.5189    0.5135       900\n",
      "\n",
      "    accuracy                         0.5083      1800\n",
      "   macro avg     0.5083    0.5083    0.5083      1800\n",
      "weighted avg     0.5083    0.5083    0.5083      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5031    0.6244    0.5573       900\n",
      "         1.0     0.5051    0.3833    0.4359       900\n",
      "\n",
      "    accuracy                         0.5039      1800\n",
      "   macro avg     0.5041    0.5039    0.4966      1800\n",
      "weighted avg     0.5041    0.5039    0.4966      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5137    0.5000    0.5068       900\n",
      "         1.0     0.5130    0.5267    0.5197       900\n",
      "\n",
      "    accuracy                         0.5133      1800\n",
      "   macro avg     0.5133    0.5133    0.5132      1800\n",
      "weighted avg     0.5133    0.5133    0.5132      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "madelon_return = cl.data_classification(train_x = Madelon_X, train_y = Madelon_Y, test_x = Madelon_x ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6724    0.8773    0.7613       269\n",
      "         1.0     0.7360    0.4444    0.5542       207\n",
      "\n",
      "    accuracy                         0.6891       476\n",
      "   macro avg     0.7042    0.6609    0.6578       476\n",
      "weighted avg     0.7000    0.6891    0.6712       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7477    0.6059    0.6694       269\n",
      "         1.0     0.5891    0.7343    0.6538       207\n",
      "\n",
      "    accuracy                         0.6618       476\n",
      "   macro avg     0.6684    0.6701    0.6616       476\n",
      "weighted avg     0.6788    0.6618    0.6626       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9004    0.9405    0.9200       269\n",
      "         1.0     0.9179    0.8647    0.8905       207\n",
      "\n",
      "    accuracy                         0.9076       476\n",
      "   macro avg     0.9092    0.9026    0.9053       476\n",
      "weighted avg     0.9080    0.9076    0.9072       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8111    0.8141    0.8126       269\n",
      "         1.0     0.7573    0.7536    0.7554       207\n",
      "\n",
      "    accuracy                         0.7878       476\n",
      "   macro avg     0.7842    0.7839    0.7840       476\n",
      "weighted avg     0.7877    0.7878    0.7878       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "musk_return = cl.data_classification(train_x = Musk_X, train_y = Musk_Y, test_x = Musk_x ,test_y = Musk_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING PROPOSED STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      " -----------RESULT # -------  0\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  80\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7085    0.8401    0.7687       269\n",
      "         1.0     0.7261    0.5507    0.6264       207\n",
      "\n",
      "    accuracy                         0.7143       476\n",
      "   macro avg     0.7173    0.6954    0.6975       476\n",
      "weighted avg     0.7161    0.7143    0.7068       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8485    0.8327    0.8405       269\n",
      "         1.0     0.7877    0.8068    0.7971       207\n",
      "\n",
      "    accuracy                         0.8214       476\n",
      "   macro avg     0.8181    0.8197    0.8188       476\n",
      "weighted avg     0.8221    0.8214    0.8217       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8490    0.9405    0.8924       269\n",
      "         1.0     0.9101    0.7826    0.8416       207\n",
      "\n",
      "    accuracy                         0.8718       476\n",
      "   macro avg     0.8796    0.8616    0.8670       476\n",
      "weighted avg     0.8756    0.8718    0.8703       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7196    0.7918    0.7540       269\n",
      "         1.0     0.6889    0.5990    0.6408       207\n",
      "\n",
      "    accuracy                         0.7080       476\n",
      "   macro avg     0.7042    0.6954    0.6974       476\n",
      "weighted avg     0.7062    0.7080    0.7048       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  1\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  91\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6963    0.8439    0.7630       269\n",
      "         1.0     0.7200    0.5217    0.6050       207\n",
      "\n",
      "    accuracy                         0.7038       476\n",
      "   macro avg     0.7082    0.6828    0.6840       476\n",
      "weighted avg     0.7066    0.7038    0.6943       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7806    0.8067    0.7934       269\n",
      "         1.0     0.7374    0.7053    0.7210       207\n",
      "\n",
      "    accuracy                         0.7626       476\n",
      "   macro avg     0.7590    0.7560    0.7572       476\n",
      "weighted avg     0.7618    0.7626    0.7619       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8715    0.9331    0.9013       269\n",
      "         1.0     0.9043    0.8213    0.8608       207\n",
      "\n",
      "    accuracy                         0.8845       476\n",
      "   macro avg     0.8879    0.8772    0.8810       476\n",
      "weighted avg     0.8858    0.8845    0.8836       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7342    0.8216    0.7754       269\n",
      "         1.0     0.7257    0.6135    0.6649       207\n",
      "\n",
      "    accuracy                         0.7311       476\n",
      "   macro avg     0.7300    0.7175    0.7202       476\n",
      "weighted avg     0.7305    0.7311    0.7274       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  2\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  126\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6959    0.8253    0.7551       269\n",
      "         1.0     0.7006    0.5314    0.6044       207\n",
      "\n",
      "    accuracy                         0.6975       476\n",
      "   macro avg     0.6983    0.6783    0.6797       476\n",
      "weighted avg     0.6980    0.6975    0.6896       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7930    0.7546    0.7733       269\n",
      "         1.0     0.7000    0.7440    0.7213       207\n",
      "\n",
      "    accuracy                         0.7500       476\n",
      "   macro avg     0.7465    0.7493    0.7473       476\n",
      "weighted avg     0.7525    0.7500    0.7507       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8750    0.9368    0.9048       269\n",
      "         1.0     0.9096    0.8261    0.8658       207\n",
      "\n",
      "    accuracy                         0.8887       476\n",
      "   macro avg     0.8923    0.8814    0.8853       476\n",
      "weighted avg     0.8900    0.8887    0.8879       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7677    0.8476    0.8057       269\n",
      "         1.0     0.7709    0.6667    0.7150       207\n",
      "\n",
      "    accuracy                         0.7689       476\n",
      "   macro avg     0.7693    0.7571    0.7603       476\n",
      "weighted avg     0.7691    0.7689    0.7662       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  3\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  89\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7040    0.8401    0.7661       269\n",
      "         1.0     0.7226    0.5411    0.6188       207\n",
      "\n",
      "    accuracy                         0.7101       476\n",
      "   macro avg     0.7133    0.6906    0.6924       476\n",
      "weighted avg     0.7121    0.7101    0.7020       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8238    0.7993    0.8113       269\n",
      "         1.0     0.7488    0.7778    0.7630       207\n",
      "\n",
      "    accuracy                         0.7899       476\n",
      "   macro avg     0.7863    0.7885    0.7872       476\n",
      "weighted avg     0.7912    0.7899    0.7903       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8591    0.9294    0.8929       269\n",
      "         1.0     0.8973    0.8019    0.8469       207\n",
      "\n",
      "    accuracy                         0.8739       476\n",
      "   macro avg     0.8782    0.8657    0.8699       476\n",
      "weighted avg     0.8757    0.8739    0.8729       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7192    0.7807    0.7487       269\n",
      "         1.0     0.6793    0.6039    0.6394       207\n",
      "\n",
      "    accuracy                         0.7038       476\n",
      "   macro avg     0.6993    0.6923    0.6940       476\n",
      "weighted avg     0.7019    0.7038    0.7011       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  153\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7170    0.8476    0.7768       269\n",
      "         1.0     0.7405    0.5652    0.6411       207\n",
      "\n",
      "    accuracy                         0.7248       476\n",
      "   macro avg     0.7287    0.7064    0.7090       476\n",
      "weighted avg     0.7272    0.7248    0.7178       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7509    0.7844    0.7673       269\n",
      "         1.0     0.7026    0.6618    0.6816       207\n",
      "\n",
      "    accuracy                         0.7311       476\n",
      "   macro avg     0.7267    0.7231    0.7244       476\n",
      "weighted avg     0.7299    0.7311    0.7300       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8630    0.9368    0.8984       269\n",
      "         1.0     0.9076    0.8068    0.8542       207\n",
      "\n",
      "    accuracy                         0.8803       476\n",
      "   macro avg     0.8853    0.8718    0.8763       476\n",
      "weighted avg     0.8824    0.8803    0.8792       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7557    0.8625    0.8056       269\n",
      "         1.0     0.7811    0.6377    0.7021       207\n",
      "\n",
      "    accuracy                         0.7647       476\n",
      "   macro avg     0.7684    0.7501    0.7538       476\n",
      "weighted avg     0.7667    0.7647    0.7606       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  5\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  122\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7009    0.8364    0.7627       269\n",
      "         1.0     0.7161    0.5362    0.6133       207\n",
      "\n",
      "    accuracy                         0.7059       476\n",
      "   macro avg     0.7085    0.6863    0.6880       476\n",
      "weighted avg     0.7075    0.7059    0.6977       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7900    0.8253    0.8073       269\n",
      "         1.0     0.7590    0.7150    0.7363       207\n",
      "\n",
      "    accuracy                         0.7773       476\n",
      "   macro avg     0.7745    0.7701    0.7718       476\n",
      "weighted avg     0.7765    0.7773    0.7764       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8750    0.9368    0.9048       269\n",
      "         1.0     0.9096    0.8261    0.8658       207\n",
      "\n",
      "    accuracy                         0.8887       476\n",
      "   macro avg     0.8923    0.8814    0.8853       476\n",
      "weighted avg     0.8900    0.8887    0.8879       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7855    0.8439    0.8136       269\n",
      "         1.0     0.7754    0.7005    0.7360       207\n",
      "\n",
      "    accuracy                         0.7815       476\n",
      "   macro avg     0.7804    0.7722    0.7748       476\n",
      "weighted avg     0.7811    0.7815    0.7799       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  6\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  68\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7085    0.8401    0.7687       269\n",
      "         1.0     0.7261    0.5507    0.6264       207\n",
      "\n",
      "    accuracy                         0.7143       476\n",
      "   macro avg     0.7173    0.6954    0.6975       476\n",
      "weighted avg     0.7161    0.7143    0.7068       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8661    0.8178    0.8413       269\n",
      "         1.0     0.7793    0.8357    0.8065       207\n",
      "\n",
      "    accuracy                         0.8256       476\n",
      "   macro avg     0.8227    0.8268    0.8239       476\n",
      "weighted avg     0.8284    0.8256    0.8262       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8514    0.9368    0.8920       269\n",
      "         1.0     0.9056    0.7874    0.8424       207\n",
      "\n",
      "    accuracy                         0.8718       476\n",
      "   macro avg     0.8785    0.8621    0.8672       476\n",
      "weighted avg     0.8749    0.8718    0.8704       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7181    0.7955    0.7549       269\n",
      "         1.0     0.6910    0.5942    0.6390       207\n",
      "\n",
      "    accuracy                         0.7080       476\n",
      "   macro avg     0.7046    0.6949    0.6969       476\n",
      "weighted avg     0.7063    0.7080    0.7045       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  7\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  134\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6939    0.8513    0.7646       269\n",
      "         1.0     0.7260    0.5121    0.6006       207\n",
      "\n",
      "    accuracy                         0.7038       476\n",
      "   macro avg     0.7100    0.6817    0.6826       476\n",
      "weighted avg     0.7079    0.7038    0.6933       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7766    0.8141    0.7949       269\n",
      "         1.0     0.7423    0.6957    0.7182       207\n",
      "\n",
      "    accuracy                         0.7626       476\n",
      "   macro avg     0.7594    0.7549    0.7566       476\n",
      "weighted avg     0.7617    0.7626    0.7616       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8596    0.9331    0.8948       269\n",
      "         1.0     0.9022    0.8019    0.8491       207\n",
      "\n",
      "    accuracy                         0.8761       476\n",
      "   macro avg     0.8809    0.8675    0.8720       476\n",
      "weighted avg     0.8781    0.8761    0.8749       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7708    0.8625    0.8140       269\n",
      "         1.0     0.7886    0.6667    0.7225       207\n",
      "\n",
      "    accuracy                         0.7773       476\n",
      "   macro avg     0.7797    0.7646    0.7683       476\n",
      "weighted avg     0.7785    0.7773    0.7742       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  8\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  83\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6966    0.8364    0.7601       269\n",
      "         1.0     0.7124    0.5266    0.6056       207\n",
      "\n",
      "    accuracy                         0.7017       476\n",
      "   macro avg     0.7045    0.6815    0.6828       476\n",
      "weighted avg     0.7035    0.7017    0.6929       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7963    0.7993    0.7978       269\n",
      "         1.0     0.7379    0.7343    0.7361       207\n",
      "\n",
      "    accuracy                         0.7710       476\n",
      "   macro avg     0.7671    0.7668    0.7669       476\n",
      "weighted avg     0.7709    0.7710    0.7709       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8601    0.9368    0.8968       269\n",
      "         1.0     0.9071    0.8019    0.8513       207\n",
      "\n",
      "    accuracy                         0.8782       476\n",
      "   macro avg     0.8836    0.8694    0.8740       476\n",
      "weighted avg     0.8805    0.8782    0.8770       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7251    0.7844    0.7536       269\n",
      "         1.0     0.6865    0.6135    0.6480       207\n",
      "\n",
      "    accuracy                         0.7101       476\n",
      "   macro avg     0.7058    0.6990    0.7008       476\n",
      "weighted avg     0.7083    0.7101    0.7076       476\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  9\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  82\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7066    0.8327    0.7645       269\n",
      "         1.0     0.7170    0.5507    0.6230       207\n",
      "\n",
      "    accuracy                         0.7101       476\n",
      "   macro avg     0.7118    0.6917    0.6937       476\n",
      "weighted avg     0.7111    0.7101    0.7029       476\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8272    0.8364    0.8318       269\n",
      "         1.0     0.7843    0.7729    0.7786       207\n",
      "\n",
      "    accuracy                         0.8088       476\n",
      "   macro avg     0.8058    0.8047    0.8052       476\n",
      "weighted avg     0.8086    0.8088    0.8087       476\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8630    0.9368    0.8984       269\n",
      "         1.0     0.9076    0.8068    0.8542       207\n",
      "\n",
      "    accuracy                         0.8803       476\n",
      "   macro avg     0.8853    0.8718    0.8763       476\n",
      "weighted avg     0.8824    0.8803    0.8792       476\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7276    0.7844    0.7549       269\n",
      "         1.0     0.6882    0.6184    0.6514       207\n",
      "\n",
      "    accuracy                         0.7122       476\n",
      "   macro avg     0.7079    0.7014    0.7032       476\n",
      "weighted avg     0.7104    0.7122    0.7099       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in range(10):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = pj1.feature_optimization(trainX = Musk_X,\n",
    "                               trainY = Musk_Y,\n",
    "                               testX = Musk_x,\n",
    "                               testY = Musk_y, \n",
    "                               population = 10,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MuskX_OP, Muskx_OP = dsp()\n",
    "    print('Number of Features ', MuskX_OP.shape[1])\n",
    "    musk_return = cl.data_classification(train_x = MuskX_OP, train_y = Musk_Y, \n",
    "                                  test_x = Muskx_OP ,test_y = Musk_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      " -----------RESULT # -------  0\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  353\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5006    0.4767    0.4883       900\n",
      "         1.0     0.5005    0.5244    0.5122       900\n",
      "\n",
      "    accuracy                         0.5006      1800\n",
      "   macro avg     0.5006    0.5006    0.5003      1800\n",
      "weighted avg     0.5006    0.5006    0.5003      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5094    0.5133    0.5113       900\n",
      "         1.0     0.5095    0.5056    0.5075       900\n",
      "\n",
      "    accuracy                         0.5094      1800\n",
      "   macro avg     0.5094    0.5094    0.5094      1800\n",
      "weighted avg     0.5094    0.5094    0.5094      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4986    0.6156    0.5510       900\n",
      "         1.0     0.4978    0.3811    0.4317       900\n",
      "\n",
      "    accuracy                         0.4983      1800\n",
      "   macro avg     0.4982    0.4983    0.4913      1800\n",
      "weighted avg     0.4982    0.4983    0.4913      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5012    0.4833    0.4921       900\n",
      "         1.0     0.5011    0.5189    0.5098       900\n",
      "\n",
      "    accuracy                         0.5011      1800\n",
      "   macro avg     0.5011    0.5011    0.5010      1800\n",
      "weighted avg     0.5011    0.5011    0.5010      1800\n",
      "\n",
      "================================\n",
      " -----------RESULT # -------  1\n",
      "Processing .....\n",
      "Only one population left\n",
      "Number of Features  387\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "classifier NB                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4994    0.4756    0.4872       900\n",
      "         1.0     0.4995    0.5233    0.5111       900\n",
      "\n",
      "    accuracy                         0.4994      1800\n",
      "   macro avg     0.4994    0.4994    0.4992      1800\n",
      "weighted avg     0.4994    0.4994    0.4992      1800\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "classifier RT                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5118    0.5056    0.5087       900\n",
      "         1.0     0.5115    0.5178    0.5146       900\n",
      "\n",
      "    accuracy                         0.5117      1800\n",
      "   macro avg     0.5117    0.5117    0.5116      1800\n",
      "weighted avg     0.5117    0.5117    0.5116      1800\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "classifier KNN                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5080    0.6378    0.5655       900\n",
      "         1.0     0.5134    0.3822    0.4382       900\n",
      "\n",
      "    accuracy                         0.5100      1800\n",
      "   macro avg     0.5107    0.5100    0.5019      1800\n",
      "weighted avg     0.5107    0.5100    0.5019      1800\n",
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "classifier LDA                precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5006    0.4911    0.4958       900\n",
      "         1.0     0.5005    0.5100    0.5052       900\n",
      "\n",
      "    accuracy                         0.5006      1800\n",
      "   macro avg     0.5006    0.5006    0.5005      1800\n",
      "weighted avg     0.5006    0.5006    0.5005      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in range(2):\n",
    "    print('================================\\n -----------RESULT # ------- ', a)\n",
    "    dsp = pj1.feature_optimization(trainX = Madelon_X,\n",
    "                               trainY = Madelon_Y,\n",
    "                               testX = Madelon_x,\n",
    "                               testY = Madelon_y, \n",
    "                               population = 30,\n",
    "                               obj_function= 'ER')\n",
    "\n",
    "    MadelonX_OP, Madelonx_OP = dsp()\n",
    "    print('Number of Features ', MadelonX_OP.shape[1])\n",
    "    Madelon_return = cl.data_classification(train_x = MadelonX_OP, train_y = Madelon_Y, \n",
    "                                         test_x = Madelonx_OP ,test_y = Madelon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
